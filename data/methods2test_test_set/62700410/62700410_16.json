{"test_class": {"identifier": "TestRecordReaderFunction", "superclass": "extends BaseSparkTest", "interfaces": "", "fields": [], "file": "datavec-spark/src/test/java/org/datavec/spark/functions/TestRecordReaderFunction.java"}, "test_case": {"identifier": "testRecordReaderFunction", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n    public void testRecordReaderFunction() throws Exception {\n\n        ClassPathResource cpr = new ClassPathResource(\"/imagetest/0/a.bmp\");\n        List<String> labelsList = Arrays.asList(\"0\", \"1\"); //Need this for Spark: can't infer without init call\n\n        String path = cpr.getFile().getAbsolutePath();\n        String folder = path.substring(0, path.length() - 7);\n        path = folder + \"*\";\n\n        JavaPairRDD<String, PortableDataStream> origData = sc.binaryFiles(path);\n        assertEquals(4, origData.count()); //4 images\n\n        ImageRecordReader irr = new ImageRecordReader(28, 28, 1, new ParentPathLabelGenerator());\n        irr.setLabels(labelsList);\n        RecordReaderFunction rrf = new RecordReaderFunction(irr);\n        JavaRDD<List<Writable>> rdd = origData.map(rrf);\n        List<List<Writable>> listSpark = rdd.collect();\n\n        assertEquals(4, listSpark.size());\n        for (int i = 0; i < 4; i++) {\n            assertEquals(1 + 1, listSpark.get(i).size());\n            assertEquals(28 * 28, ((ArrayWritable) listSpark.get(i).iterator().next()).length());\n        }\n\n        //Load normally (i.e., not via Spark), and check that we get the same results (order not withstanding)\n        InputSplit is = new FileSplit(new File(folder), new String[] {\"bmp\"}, true);\n        //        System.out.println(\"Locations: \" + Arrays.toString(is.locations()));\n        irr = new ImageRecordReader(28, 28, 1, new ParentPathLabelGenerator());\n        irr.initialize(is);\n\n        List<List<Writable>> list = new ArrayList<>(4);\n        while (irr.hasNext()) {\n            list.add(irr.next());\n        }\n        assertEquals(4, list.size());\n\n        //        System.out.println(\"Spark list:\");\n        //        for(List<Writable> c : listSpark ) System.out.println(c);\n        //        System.out.println(\"Local list:\");\n        //        for(List<Writable> c : list ) System.out.println(c);\n\n        //Check that each of the values from Spark equals exactly one of the values doing it locally\n        boolean[] found = new boolean[4];\n        for (int i = 0; i < 4; i++) {\n            int foundIndex = -1;\n            List<Writable> collection = listSpark.get(i);\n            for (int j = 0; j < 4; j++) {\n                if (collection.equals(list.get(j))) {\n                    if (foundIndex != -1)\n                        fail(); //Already found this value -> suggests this spark value equals two or more of local version? (Shouldn't happen)\n                    foundIndex = j;\n                    if (found[foundIndex])\n                        fail(); //One of the other spark values was equal to this one -> suggests duplicates in Spark list\n                    found[foundIndex] = true; //mark this one as seen before\n                }\n            }\n        }\n        int count = 0;\n        for (boolean b : found)\n            if (b)\n                count++;\n        assertEquals(4, count); //Expect all 4 and exactly 4 pairwise matches between spark and local versions\n    }", "signature": "void testRecordReaderFunction()", "full_signature": "@Test public void testRecordReaderFunction()", "class_method_signature": "TestRecordReaderFunction.testRecordReaderFunction()", "testcase": true, "constructor": false, "invocations": ["asList", "getAbsolutePath", "getFile", "substring", "length", "binaryFiles", "assertEquals", "count", "setLabels", "map", "collect", "assertEquals", "size", "assertEquals", "size", "get", "assertEquals", "length", "next", "iterator", "get", "initialize", "hasNext", "add", "next", "assertEquals", "size", "get", "equals", "get", "fail", "fail", "assertEquals"]}, "focal_class": {"identifier": "RecordReaderFunction", "superclass": "", "interfaces": "implements Function<Tuple2<String, PortableDataStream>, List<Writable>>", "fields": [{"original_string": "protected RecordReader recordReader;", "modifier": "protected", "type": "RecordReader", "declarator": "recordReader", "var_name": "recordReader"}], "methods": [{"identifier": "RecordReaderFunction", "parameters": "(RecordReader recordReader)", "modifiers": "public", "return": "", "signature": " RecordReaderFunction(RecordReader recordReader)", "full_signature": "public  RecordReaderFunction(RecordReader recordReader)", "class_method_signature": "RecordReaderFunction.RecordReaderFunction(RecordReader recordReader)", "testcase": false, "constructor": true}, {"identifier": "call", "parameters": "(Tuple2<String, PortableDataStream> value)", "modifiers": "@Override public", "return": "List<Writable>", "signature": "List<Writable> call(Tuple2<String, PortableDataStream> value)", "full_signature": "@Override public List<Writable> call(Tuple2<String, PortableDataStream> value)", "class_method_signature": "RecordReaderFunction.call(Tuple2<String, PortableDataStream> value)", "testcase": false, "constructor": false}], "file": "datavec-spark/src/main/java/org/datavec/spark/functions/RecordReaderFunction.java"}, "focal_method": {"identifier": "RecordReaderFunction", "parameters": "(RecordReader recordReader)", "modifiers": "public", "return": "", "body": "public RecordReaderFunction(RecordReader recordReader) {\n        this.recordReader = recordReader;\n    }", "signature": " RecordReaderFunction(RecordReader recordReader)", "full_signature": "public  RecordReaderFunction(RecordReader recordReader)", "class_method_signature": "RecordReaderFunction.RecordReaderFunction(RecordReader recordReader)", "testcase": false, "constructor": true, "invocations": []}, "repository": {"repo_id": 62700410, "url": "https://github.com/deeplearning4j/DataVec", "language": "Java", "is_fork": false, "fork_count": 174, "stargazer_count": 260, "size": 39842, "license": "licensed"}}