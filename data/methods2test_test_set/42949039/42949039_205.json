{"test_class": {"identifier": "Hive13DdlGeneratorTest", "superclass": "extends AbstractServiceTest", "interfaces": "", "fields": [], "file": "herd-code/herd-service/src/test/java/org/finra/herd/service/helper/Hive13DdlGeneratorTest.java"}, "test_case": {"identifier": "testGetHivePartitionEmptyPartitions", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n    public void testGetHivePartitionEmptyPartitions()\n    {\n        List<SchemaColumn> autoDiscoverableSubPartitionColumns;\n        List<String> storageFilePaths;\n        List<HivePartitionDto> expectedHivePartitions;\n        List<HivePartitionDto> resultHivePartitions;\n\n        // Create a business object data key without any sub-partitions.\n        BusinessObjectDataKey businessObjectDataKey =\n            new BusinessObjectDataKey(NAMESPACE, BDEF_NAME, FORMAT_USAGE_CODE, FORMAT_FILE_TYPE_CODE, FORMAT_VERSION, PARTITION_VALUE, NO_SUBPARTITION_VALUES,\n                DATA_VERSION);\n\n        // No auto-discoverable sub-partition columns with empty partition represented by \"/\".\n        autoDiscoverableSubPartitionColumns = new ArrayList<>();\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"/\"));\n        expectedHivePartitions =\n            Collections.singletonList(HivePartitionDto.builder().withPath(\"\").withPartitionValues(Collections.singletonList(PARTITION_VALUE)).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // No auto-discoverable sub-partition columns with empty partition represented by \"_$folder$\".\n        autoDiscoverableSubPartitionColumns = new ArrayList<>();\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"_$folder$\"));\n        expectedHivePartitions =\n            Collections.singletonList(HivePartitionDto.builder().withPath(\"\").withPartitionValues(Collections.singletonList(PARTITION_VALUE)).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Single sub-partition column with empty partition represented by \"/\".\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Collections.singletonList(\"column1\"));\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"/column1=aa/\"));\n        expectedHivePartitions =\n            Collections.singletonList(HivePartitionDto.builder().withPath(\"/column1=aa\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Single sub-partition column with empty partition represented by \"_$folder$\".\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Collections.singletonList(\"column1\"));\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"/column1=aa_$folder$\"));\n        expectedHivePartitions =\n            Collections.singletonList(HivePartitionDto.builder().withPath(\"/column1=aa\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Two sub-partition columns with empty partition represented by \"/\".\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Arrays.asList(\"Column1\", \"column2\"));\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"/column1=aa/column2=bb/\"));\n        expectedHivePartitions = Collections.singletonList(\n            HivePartitionDto.builder().withPath(\"/column1=aa/column2=bb\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\", \"bb\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Two sub-partition columns with empty partition represented by \"_$folder$\".\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Arrays.asList(\"Column1\", \"column2\"));\n        storageFilePaths = getStorageFilePaths(Collections.singletonList(\"/column1=aa/column2=bb_$folder$\"));\n        expectedHivePartitions = Collections.singletonList(\n            HivePartitionDto.builder().withPath(\"/column1=aa/column2=bb\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\", \"bb\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Two sub-partition columns with empty partition represented by \"/\" and with data files present.\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Arrays.asList(\"Column1\", \"column2\"));\n        storageFilePaths = getStorageFilePaths(Arrays.asList(\"/column1=aa/column2=bb/\", \"/column1=aa/column2=bb/file.dat\"));\n        expectedHivePartitions = Collections.singletonList(\n            HivePartitionDto.builder().withPath(\"/column1=aa/column2=bb\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\", \"bb\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Two sub-partition columns with empty partition represented by \"_$folder$\" and with data files present.\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Arrays.asList(\"Column1\", \"column2\"));\n        storageFilePaths = getStorageFilePaths(Arrays.asList(\"/column1=aa/column2=bb_$folder$\", \"/column1=aa/column2=bb/file.dat\"));\n        expectedHivePartitions = Collections.singletonList(\n            HivePartitionDto.builder().withPath(\"/column1=aa/column2=bb\").withPartitionValues(Arrays.asList(PARTITION_VALUE, \"aa\", \"bb\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n\n        // Maximum supported number of sub-partition columns with empty folders present on every level and with data files present at the last sub-partition.\n        // Different partition values are used to validate discovery logic for the fully qualified partitions.\n        // There are also the following two special cases added:\n        // - sub-partitions added that contain empty folder marker as a sub-partition value, which is allowed\n        // - sub-partitions with data files that start, contain, and/or end with an empty folder marker\n        autoDiscoverableSubPartitionColumns = getPartitionColumns(Arrays.asList(\"Column_1\", \"Column_2\", \"Column_3\", \"Column_4\"));\n        storageFilePaths = getStorageFilePaths(Arrays.asList(\"/\",                       // an empty folder for the primary partition\n            \"_$folder$\",                                                                // an empty folder for the primary partition\n            \"/column_1=a01/\",                                                           // an empty folder for the first sub-partition\n            \"/column_1=a02_$folder$\",                                                   // an empty folder for the first sub-partition\n            \"/column_1=a03/column-2=b01/\",                                              // an empty folder for the second sub-partition\n            \"/column_1=a04/column-2=b02_$folder$\",                                      // an empty folder for the second sub-partition\n            \"/column_1=a05/column-2=b03/COLUMN_3=c01/\",                                 // an empty folder for the third sub-partition\n            \"/column_1=a06/column-2=b04/COLUMN_3=c02_$folder$\",                         // an empty folder for the third sub-partition\n            \"/column_1=a07/column-2=b05/COLUMN_3=c03/COLUMN-4=d01/\",                    // an empty folder for the forth sub-partition\n            \"/column_1=a08/column-2=b06/COLUMN_3=c04/COLUMN-4=d02_$folder$\",            // an empty folder for the forth sub-partition\n            \"/column_1=a09/column-2=b07/COLUMN_3=c05/COLUMN-4=d03/file.dat\",            // a data file present in the forth sub-partition\n            \"/column_1=a10/column-2=b08/COLUMN_3=c06/COLUMN-4=d04_$folder$/\",           // a sub-partition value is an empty folder marker\n            \"/column_1=a11/column-2=b09/COLUMN_3=c07/COLUMN-4=d05_$folder$/file.dat\",   // a sub-partition value is an empty folder marker\n            \"/column_1=a12/column-2=b10/COLUMN_3=c08/COLUMN-4=d06/_$folder$file.dat\",   // data file name starts an empty folder marker\n            \"/column_1=a13/column-2=b11/COLUMN_3=c09/COLUMN-4=d07/file_$folder$.dat\",   // data file name contains an empty folder marker\n            \"/column_1=a14/column-2=b12/COLUMN_3=c10/COLUMN-4=d08/file.dat_$folder$\",   // data file name ends with an empty folder marker\n            \"/column_1=a15/column-2=b13/COLUMN_3=c11/COLUMN-4=d09/_$folder$\"            // data file name is an empty folder marker\n        ));\n        expectedHivePartitions = Arrays.asList(HivePartitionDto.builder().withPath(\"/column_1=a07/column-2=b05/COLUMN_3=c03/COLUMN-4=d01\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a07\", \"b05\", \"c03\", \"d01\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a08/column-2=b06/COLUMN_3=c04/COLUMN-4=d02\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a08\", \"b06\", \"c04\", \"d02\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a09/column-2=b07/COLUMN_3=c05/COLUMN-4=d03\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a09\", \"b07\", \"c05\", \"d03\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a10/column-2=b08/COLUMN_3=c06/COLUMN-4=d04_$folder$\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a10\", \"b08\", \"c06\", \"d04_$folder$\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a11/column-2=b09/COLUMN_3=c07/COLUMN-4=d05_$folder$\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a11\", \"b09\", \"c07\", \"d05_$folder$\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a12/column-2=b10/COLUMN_3=c08/COLUMN-4=d06\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a12\", \"b10\", \"c08\", \"d06\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a13/column-2=b11/COLUMN_3=c09/COLUMN-4=d07\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a13\", \"b11\", \"c09\", \"d07\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a14/column-2=b12/COLUMN_3=c10/COLUMN-4=d08\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a14\", \"b12\", \"c10\", \"d08\")).build(),\n            HivePartitionDto.builder().withPath(\"/column_1=a15/column-2=b13/COLUMN_3=c11/COLUMN-4=d09\")\n                .withPartitionValues(Arrays.asList(PARTITION_VALUE, \"a15\", \"b13\", \"c11\", \"d09\")).build());\n        resultHivePartitions = hive13DdlGenerator\n            .getHivePartitions(businessObjectDataKey, autoDiscoverableSubPartitionColumns, TEST_S3_KEY_PREFIX, storageFilePaths, STORAGE_NAME);\n        assertEquals(expectedHivePartitions, resultHivePartitions);\n    }", "signature": "void testGetHivePartitionEmptyPartitions()", "full_signature": "@Test public void testGetHivePartitionEmptyPartitions()", "class_method_signature": "Hive13DdlGeneratorTest.testGetHivePartitionEmptyPartitions()", "testcase": true, "constructor": false, "invocations": ["getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "singletonList", "getHivePartitions", "assertEquals", "getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "singletonList", "getHivePartitions", "assertEquals", "getPartitionColumns", "singletonList", "getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "singletonList", "getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "asList", "getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "asList", "getStorageFilePaths", "singletonList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "asList", "getStorageFilePaths", "asList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "asList", "getStorageFilePaths", "asList", "singletonList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals", "getPartitionColumns", "asList", "getStorageFilePaths", "asList", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "build", "withPartitionValues", "withPath", "builder", "asList", "getHivePartitions", "assertEquals"]}, "focal_class": {"identifier": "Hive13DdlGenerator", "superclass": "extends DdlGenerator", "interfaces": "", "fields": [{"original_string": "public static final String NO_PARTITIONING_PARTITION_KEY = \"partition\";", "modifier": "public static final", "type": "String", "declarator": "NO_PARTITIONING_PARTITION_KEY = \"partition\"", "var_name": "NO_PARTITIONING_PARTITION_KEY"}, {"original_string": "public static final String NO_PARTITIONING_PARTITION_VALUE = \"none\";", "modifier": "public static final", "type": "String", "declarator": "NO_PARTITIONING_PARTITION_VALUE = \"none\"", "var_name": "NO_PARTITIONING_PARTITION_VALUE"}, {"original_string": "public static final String ORC_HIVE_FILE_FORMAT = \"ORC\";", "modifier": "public static final", "type": "String", "declarator": "ORC_HIVE_FILE_FORMAT = \"ORC\"", "var_name": "ORC_HIVE_FILE_FORMAT"}, {"original_string": "public static final String PARQUET_HIVE_FILE_FORMAT = \"PARQUET\";", "modifier": "public static final", "type": "String", "declarator": "PARQUET_HIVE_FILE_FORMAT = \"PARQUET\"", "var_name": "PARQUET_HIVE_FILE_FORMAT"}, {"original_string": "public static final String TEXT_HIVE_FILE_FORMAT = \"TEXTFILE\";", "modifier": "public static final", "type": "String", "declarator": "TEXT_HIVE_FILE_FORMAT = \"TEXTFILE\"", "var_name": "TEXT_HIVE_FILE_FORMAT"}, {"original_string": "public static final String JSON_HIVE_FILE_FORMAT = \"JSONFILE\";", "modifier": "public static final", "type": "String", "declarator": "JSON_HIVE_FILE_FORMAT = \"JSONFILE\"", "var_name": "JSON_HIVE_FILE_FORMAT"}, {"original_string": "public static final String REGEX_S3_EMPTY_PARTITION = \"_\\\\$folder\\\\$\";", "modifier": "public static final", "type": "String", "declarator": "REGEX_S3_EMPTY_PARTITION = \"_\\\\$folder\\\\$\"", "var_name": "REGEX_S3_EMPTY_PARTITION"}, {"original_string": "private static final List<String> HIVE_COMPLEX_DATA_TYPES =\n        Arrays.asList(Category.LIST.toString(), Category.MAP.toString(), Category.UNION.toString(), Category.STRUCT.toString());", "modifier": "private static final", "type": "List<String>", "declarator": "HIVE_COMPLEX_DATA_TYPES =\n        Arrays.asList(Category.LIST.toString(), Category.MAP.toString(), Category.UNION.toString(), Category.STRUCT.toString())", "var_name": "HIVE_COMPLEX_DATA_TYPES"}, {"original_string": "@Autowired\n    private BusinessObjectDataDdlPartitionsHelper businessObjectDataDdlPartitionsHelper;", "modifier": "@Autowired\n    private", "type": "BusinessObjectDataDdlPartitionsHelper", "declarator": "businessObjectDataDdlPartitionsHelper", "var_name": "businessObjectDataDdlPartitionsHelper"}, {"original_string": "@Autowired\n    private BusinessObjectDataHelper businessObjectDataHelper;", "modifier": "@Autowired\n    private", "type": "BusinessObjectDataHelper", "declarator": "businessObjectDataHelper", "var_name": "businessObjectDataHelper"}, {"original_string": "@Autowired\n    private BusinessObjectFormatHelper businessObjectFormatHelper;", "modifier": "@Autowired\n    private", "type": "BusinessObjectFormatHelper", "declarator": "businessObjectFormatHelper", "var_name": "businessObjectFormatHelper"}], "methods": [{"identifier": "escapeSingleQuotes", "parameters": "(String string)", "modifiers": "public", "return": "String", "signature": "String escapeSingleQuotes(String string)", "full_signature": "public String escapeSingleQuotes(String string)", "class_method_signature": "Hive13DdlGenerator.escapeSingleQuotes(String string)", "testcase": false, "constructor": false}, {"identifier": "generateCreateTableDdl", "parameters": "(BusinessObjectDataDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity, List<String> storageNames, List<StorageEntity> requestedStorageEntities,\n        Map<String, StorageEntity> cachedStorageEntities, Map<String, String> cachedS3BucketNames)", "modifiers": "@Override public", "return": "String", "signature": "String generateCreateTableDdl(BusinessObjectDataDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity, List<String> storageNames, List<StorageEntity> requestedStorageEntities,\n        Map<String, StorageEntity> cachedStorageEntities, Map<String, String> cachedS3BucketNames)", "full_signature": "@Override public String generateCreateTableDdl(BusinessObjectDataDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity, List<String> storageNames, List<StorageEntity> requestedStorageEntities,\n        Map<String, StorageEntity> cachedStorageEntities, Map<String, String> cachedS3BucketNames)", "class_method_signature": "Hive13DdlGenerator.generateCreateTableDdl(BusinessObjectDataDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity, List<String> storageNames, List<StorageEntity> requestedStorageEntities,\n        Map<String, StorageEntity> cachedStorageEntities, Map<String, String> cachedS3BucketNames)", "testcase": false, "constructor": false}, {"identifier": "generateCreateTableDdl", "parameters": "(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity)", "modifiers": "@Override public", "return": "String", "signature": "String generateCreateTableDdl(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity)", "full_signature": "@Override public String generateCreateTableDdl(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity)", "class_method_signature": "Hive13DdlGenerator.generateCreateTableDdl(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity,\n        CustomDdlEntity customDdlEntity)", "testcase": false, "constructor": false}, {"identifier": "generateReplaceColumnsStatement", "parameters": "(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity)", "modifiers": "@Override public", "return": "String", "signature": "String generateReplaceColumnsStatement(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity)", "full_signature": "@Override public String generateReplaceColumnsStatement(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity)", "class_method_signature": "Hive13DdlGenerator.generateReplaceColumnsStatement(BusinessObjectFormatDdlRequest request, BusinessObjectFormatEntity businessObjectFormatEntity)", "testcase": false, "constructor": false}, {"identifier": "getDdlCharacterValue", "parameters": "(String string)", "modifiers": "public", "return": "String", "signature": "String getDdlCharacterValue(String string)", "full_signature": "public String getDdlCharacterValue(String string)", "class_method_signature": "Hive13DdlGenerator.getDdlCharacterValue(String string)", "testcase": false, "constructor": false}, {"identifier": "getDdlCharacterValue", "parameters": "(String string, boolean escapeSingleBackslash)", "modifiers": "public", "return": "String", "signature": "String getDdlCharacterValue(String string, boolean escapeSingleBackslash)", "full_signature": "public String getDdlCharacterValue(String string, boolean escapeSingleBackslash)", "class_method_signature": "Hive13DdlGenerator.getDdlCharacterValue(String string, boolean escapeSingleBackslash)", "testcase": false, "constructor": false}, {"identifier": "getDdlOutputFormat", "parameters": "()", "modifiers": "@Override public", "return": "BusinessObjectDataDdlOutputFormatEnum", "signature": "BusinessObjectDataDdlOutputFormatEnum getDdlOutputFormat()", "full_signature": "@Override public BusinessObjectDataDdlOutputFormatEnum getDdlOutputFormat()", "class_method_signature": "Hive13DdlGenerator.getDdlOutputFormat()", "testcase": false, "constructor": false}, {"identifier": "getHivePartitions", "parameters": "(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "modifiers": "public", "return": "List<HivePartitionDto>", "signature": "List<HivePartitionDto> getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "full_signature": "public List<HivePartitionDto> getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "class_method_signature": "Hive13DdlGenerator.getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "testcase": false, "constructor": false}, {"identifier": "getHivePathPattern", "parameters": "(List<SchemaColumn> partitionColumns)", "modifiers": "public", "return": "Pattern", "signature": "Pattern getHivePathPattern(List<SchemaColumn> partitionColumns)", "full_signature": "public Pattern getHivePathPattern(List<SchemaColumn> partitionColumns)", "class_method_signature": "Hive13DdlGenerator.getHivePathPattern(List<SchemaColumn> partitionColumns)", "testcase": false, "constructor": false}, {"identifier": "getHivePathRegex", "parameters": "(List<SchemaColumn> partitionColumns)", "modifiers": "public", "return": "String", "signature": "String getHivePathRegex(List<SchemaColumn> partitionColumns)", "full_signature": "public String getHivePathRegex(List<SchemaColumn> partitionColumns)", "class_method_signature": "Hive13DdlGenerator.getHivePathRegex(List<SchemaColumn> partitionColumns)", "testcase": false, "constructor": false}, {"identifier": "assertSchemaColumnsNotEmpty", "parameters": "(BusinessObjectFormat businessObjectFormat, BusinessObjectFormatEntity businessObjectFormatEntity)", "modifiers": "private", "return": "void", "signature": "void assertSchemaColumnsNotEmpty(BusinessObjectFormat businessObjectFormat, BusinessObjectFormatEntity businessObjectFormatEntity)", "full_signature": "private void assertSchemaColumnsNotEmpty(BusinessObjectFormat businessObjectFormat, BusinessObjectFormatEntity businessObjectFormatEntity)", "class_method_signature": "Hive13DdlGenerator.assertSchemaColumnsNotEmpty(BusinessObjectFormat businessObjectFormat, BusinessObjectFormatEntity businessObjectFormatEntity)", "testcase": false, "constructor": false}, {"identifier": "generateCreateTableDdlHelper", "parameters": "(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest)", "modifiers": "private", "return": "String", "signature": "String generateCreateTableDdlHelper(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest)", "full_signature": "private String generateCreateTableDdlHelper(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest)", "class_method_signature": "Hive13DdlGenerator.generateCreateTableDdlHelper(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest)", "testcase": false, "constructor": false}, {"identifier": "generateDdlColumns", "parameters": "(BusinessObjectFormatEntity businessObjectFormatEntity, BusinessObjectFormat businessObjectFormat)", "modifiers": "private", "return": "String", "signature": "String generateDdlColumns(BusinessObjectFormatEntity businessObjectFormatEntity, BusinessObjectFormat businessObjectFormat)", "full_signature": "private String generateDdlColumns(BusinessObjectFormatEntity businessObjectFormatEntity, BusinessObjectFormat businessObjectFormat)", "class_method_signature": "Hive13DdlGenerator.generateDdlColumns(BusinessObjectFormatEntity businessObjectFormatEntity, BusinessObjectFormat businessObjectFormat)", "testcase": false, "constructor": false}, {"identifier": "generateStandardBaseDdl", "parameters": "(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "modifiers": "private", "return": "void", "signature": "void generateStandardBaseDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "full_signature": "private void generateStandardBaseDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "class_method_signature": "Hive13DdlGenerator.generateStandardBaseDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "testcase": false, "constructor": false}, {"identifier": "getHiveDataType", "parameters": "(SchemaColumn schemaColumn, BusinessObjectFormatEntity businessObjectFormatEntity)", "modifiers": "private", "return": "String", "signature": "String getHiveDataType(SchemaColumn schemaColumn, BusinessObjectFormatEntity businessObjectFormatEntity)", "full_signature": "private String getHiveDataType(SchemaColumn schemaColumn, BusinessObjectFormatEntity businessObjectFormatEntity)", "class_method_signature": "Hive13DdlGenerator.getHiveDataType(SchemaColumn schemaColumn, BusinessObjectFormatEntity businessObjectFormatEntity)", "testcase": false, "constructor": false}, {"identifier": "isHiveComplexDataType", "parameters": "(String inputString)", "modifiers": "private", "return": "boolean", "signature": "boolean isHiveComplexDataType(String inputString)", "full_signature": "private boolean isHiveComplexDataType(String inputString)", "class_method_signature": "Hive13DdlGenerator.isHiveComplexDataType(String inputString)", "testcase": false, "constructor": false}, {"identifier": "getHiveFileFormat", "parameters": "(BusinessObjectFormatEntity businessObjectFormatEntity)", "modifiers": "private", "return": "String", "signature": "String getHiveFileFormat(BusinessObjectFormatEntity businessObjectFormatEntity)", "full_signature": "private String getHiveFileFormat(BusinessObjectFormatEntity businessObjectFormatEntity)", "class_method_signature": "Hive13DdlGenerator.getHiveFileFormat(BusinessObjectFormatEntity businessObjectFormatEntity)", "testcase": false, "constructor": false}, {"identifier": "processPartitionFiltersForGenerateDdl", "parameters": "(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        HashMap<String, String> replacements, BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "modifiers": "private", "return": "void", "signature": "void processPartitionFiltersForGenerateDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        HashMap<String, String> replacements, BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "full_signature": "private void processPartitionFiltersForGenerateDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        HashMap<String, String> replacements, BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "class_method_signature": "Hive13DdlGenerator.processPartitionFiltersForGenerateDdl(BusinessObjectDataDdlPartitionsHelper.GenerateDdlRequestWrapper generateDdlRequest, StringBuilder sb,\n        HashMap<String, String> replacements, BusinessObjectFormat businessObjectFormat, String ifNotExistsOption)", "testcase": false, "constructor": false}], "file": "herd-code/herd-service/src/main/java/org/finra/herd/service/helper/Hive13DdlGenerator.java"}, "focal_method": {"identifier": "getHivePartitions", "parameters": "(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "modifiers": "public", "return": "List<HivePartitionDto>", "body": "public List<HivePartitionDto> getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)\n    {\n        // We are using linked hash map to preserve the order of the discovered partitions.\n        LinkedHashMap<List<String>, HivePartitionDto> linkedHashMap = new LinkedHashMap<>();\n\n        Pattern pattern = getHivePathPattern(autoDiscoverableSubPartitionColumns);\n        for (String storageFile : storageFiles)\n        {\n            // Remove S3 key prefix from the file path. Please note that the storage files are already validated to start with S3 key prefix.\n            String relativeFilePath = storageFile.substring(s3KeyPrefix.length());\n\n            // Try to match the relative file path to the expected subpartition folders.\n            Matcher matcher = pattern.matcher(relativeFilePath);\n            Assert.isTrue(matcher.matches(), String.format(\"Registered storage file or directory does not match the expected Hive sub-directory pattern. \" +\n                    \"Storage: {%s}, file/directory: {%s}, business object data: {%s}, S3 key prefix: {%s}, pattern: {%s}\", storageName, storageFile,\n                businessObjectDataHelper.businessObjectDataKeyToString(businessObjectDataKey), s3KeyPrefix, pattern.pattern()));\n\n            // Create a list of partition values.\n            List<String> partitionValues = new ArrayList<>();\n\n            // Add partition values per business object data key.\n            partitionValues.add(businessObjectDataKey.getPartitionValue());\n            partitionValues.addAll(businessObjectDataKey.getSubPartitionValues());\n\n            // Extract relative partition values.\n            for (int i = 1; i <= matcher.groupCount() - 1; i++)\n            {\n                partitionValues.add(matcher.group(i));\n            }\n\n            // Get the matched trailing folder markers and an optional file name.\n            String partitionPathTrailingPart = matcher.group(matcher.groupCount());\n\n            // If we did not match all expected partition values along with the trailing folder markers and an optional file name, then this storage file\n            // path is not part of a fully qualified partition (this is an intermediate folder marker) and we ignore it.\n            if (!partitionValues.contains(null))\n            {\n                // Get path for this partition by removing trailing \"/\" followed by an optional file name or \"_$folder$\" which represents an empty folder in S3.\n                String partitionPath = relativeFilePath.substring(0, relativeFilePath.length() - StringUtils.length(partitionPathTrailingPart));\n\n                // Check if we already have that partition discovered - that would happen if partition contains multiple data files.\n                HivePartitionDto hivePartition = linkedHashMap.get(partitionValues);\n\n                if (hivePartition != null)\n                {\n                    // Partition is already discovered, so just validate that the relative paths match.\n                    Assert.isTrue(hivePartition.getPath().equals(partitionPath), String.format(\n                        \"Found two different locations for the same Hive partition. Storage: {%s}, business object data: {%s}, \" +\n                            \"S3 key prefix: {%s}, path[1]: {%s}, path[2]: {%s}\", storageName,\n                        businessObjectDataHelper.businessObjectDataKeyToString(businessObjectDataKey), s3KeyPrefix, hivePartition.getPath(), partitionPath));\n                }\n                else\n                {\n                    // Add this partition to the hash map of discovered partitions.\n                    linkedHashMap.put(partitionValues, new HivePartitionDto(partitionPath, partitionValues));\n                }\n            }\n        }\n\n        List<HivePartitionDto> hivePartitions = new ArrayList<>();\n        hivePartitions.addAll(linkedHashMap.values());\n\n        return hivePartitions;\n    }", "signature": "List<HivePartitionDto> getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "full_signature": "public List<HivePartitionDto> getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "class_method_signature": "Hive13DdlGenerator.getHivePartitions(BusinessObjectDataKey businessObjectDataKey, List<SchemaColumn> autoDiscoverableSubPartitionColumns,\n        String s3KeyPrefix, Collection<String> storageFiles, String storageName)", "testcase": false, "constructor": false, "invocations": ["getHivePathPattern", "substring", "length", "matcher", "isTrue", "matches", "format", "businessObjectDataKeyToString", "pattern", "add", "getPartitionValue", "addAll", "getSubPartitionValues", "groupCount", "add", "group", "group", "groupCount", "contains", "substring", "length", "length", "get", "isTrue", "equals", "getPath", "format", "businessObjectDataKeyToString", "getPath", "put", "addAll", "values"]}, "repository": {"repo_id": 42949039, "url": "https://github.com/FINRAOS/herd", "language": "Java", "is_fork": false, "fork_count": 36, "stargazer_count": 120, "size": 205797, "license": "licensed"}}