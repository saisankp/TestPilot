{"test_class": {"identifier": "TestKafkaSource", "superclass": "", "interfaces": "", "fields": [{"original_string": "private static final Logger log = LoggerFactory.getLogger(TestKafkaSource.class);", "modifier": "private static final", "type": "Logger", "declarator": "log = LoggerFactory.getLogger(TestKafkaSource.class)", "var_name": "log"}, {"original_string": "private KafkaSource kafkaSource;", "modifier": "private", "type": "KafkaSource", "declarator": "kafkaSource", "var_name": "kafkaSource"}, {"original_string": "private static KafkaSourceEmbeddedKafka kafkaServer;", "modifier": "private static", "type": "KafkaSourceEmbeddedKafka", "declarator": "kafkaServer", "var_name": "kafkaServer"}, {"original_string": "private Context context;", "modifier": "private", "type": "Context", "declarator": "context", "var_name": "context"}, {"original_string": "private List<Event> events;", "modifier": "private", "type": "List<Event>", "declarator": "events", "var_name": "events"}, {"original_string": "private final List<String> usedTopics = new ArrayList<>();", "modifier": "private final", "type": "List<String>", "declarator": "usedTopics = new ArrayList<>()", "var_name": "usedTopics"}, {"original_string": "private String topic0;", "modifier": "private", "type": "String", "declarator": "topic0", "var_name": "topic0"}, {"original_string": "private String topic1;", "modifier": "private", "type": "String", "declarator": "topic1", "var_name": "topic1"}], "file": "flume-ng-sources/flume-kafka-source/src/test/java/org/apache/flume/source/kafka/TestKafkaSource.java"}, "test_case": {"identifier": "testErrorCounters", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void testErrorCounters() throws InterruptedException, EventDeliveryException {\n    context.put(TOPICS, topic0);\n    context.put(BATCH_SIZE, \"1\");\n    kafkaSource.configure(context);\n\n    ChannelProcessor cp = Mockito.mock(ChannelProcessor.class);\n    doThrow(new ChannelException(\"dummy\")).doThrow(new RuntimeException(\"dummy\"))\n        .when(cp).processEventBatch(any(List.class));\n    kafkaSource.setChannelProcessor(cp);\n\n    startKafkaSource();\n\n    Thread.sleep(500L);\n\n    kafkaServer.produce(topic0, \"\", \"hello, world\");\n\n    Thread.sleep(500L);\n\n    kafkaSource.doProcess();\n    kafkaSource.doProcess();\n\n    SourceCounter sc = (SourceCounter) Whitebox.getInternalState(kafkaSource, \"counter\");\n    Assert.assertEquals(1, sc.getChannelWriteFail());\n    Assert.assertEquals(1, sc.getEventReadFail());\n\n    kafkaSource.stop();\n  }", "signature": "void testErrorCounters()", "full_signature": "@Test public void testErrorCounters()", "class_method_signature": "TestKafkaSource.testErrorCounters()", "testcase": true, "constructor": false, "invocations": ["put", "put", "configure", "mock", "processEventBatch", "when", "doThrow", "doThrow", "any", "setChannelProcessor", "startKafkaSource", "sleep", "produce", "sleep", "doProcess", "doProcess", "getInternalState", "assertEquals", "getChannelWriteFail", "assertEquals", "getEventReadFail", "stop"]}, "focal_class": {"identifier": "KafkaSource", "superclass": "extends AbstractPollableSource", "interfaces": "implements Configurable, BatchSizeSupported", "fields": [{"original_string": "private static final Logger log = LoggerFactory.getLogger(KafkaSource.class);", "modifier": "private static final", "type": "Logger", "declarator": "log = LoggerFactory.getLogger(KafkaSource.class)", "var_name": "log"}, {"original_string": "private static final int ZK_SESSION_TIMEOUT = 30000;", "modifier": "private static final", "type": "int", "declarator": "ZK_SESSION_TIMEOUT = 30000", "var_name": "ZK_SESSION_TIMEOUT"}, {"original_string": "private static final int ZK_CONNECTION_TIMEOUT = 30000;", "modifier": "private static final", "type": "int", "declarator": "ZK_CONNECTION_TIMEOUT = 30000", "var_name": "ZK_CONNECTION_TIMEOUT"}, {"original_string": "private Context context;", "modifier": "private", "type": "Context", "declarator": "context", "var_name": "context"}, {"original_string": "private Properties kafkaProps;", "modifier": "private", "type": "Properties", "declarator": "kafkaProps", "var_name": "kafkaProps"}, {"original_string": "private KafkaSourceCounter counter;", "modifier": "private", "type": "KafkaSourceCounter", "declarator": "counter", "var_name": "counter"}, {"original_string": "private KafkaConsumer<String, byte[]> consumer;", "modifier": "private", "type": "KafkaConsumer<String, byte[]>", "declarator": "consumer", "var_name": "consumer"}, {"original_string": "private Iterator<ConsumerRecord<String, byte[]>> it;", "modifier": "private", "type": "Iterator<ConsumerRecord<String, byte[]>>", "declarator": "it", "var_name": "it"}, {"original_string": "private final List<Event> eventList = new ArrayList<Event>();", "modifier": "private final", "type": "List<Event>", "declarator": "eventList = new ArrayList<Event>()", "var_name": "eventList"}, {"original_string": "private Map<TopicPartition, OffsetAndMetadata> tpAndOffsetMetadata;", "modifier": "private", "type": "Map<TopicPartition, OffsetAndMetadata>", "declarator": "tpAndOffsetMetadata", "var_name": "tpAndOffsetMetadata"}, {"original_string": "private AtomicBoolean rebalanceFlag;", "modifier": "private", "type": "AtomicBoolean", "declarator": "rebalanceFlag", "var_name": "rebalanceFlag"}, {"original_string": "private Map<String, String> headers;", "modifier": "private", "type": "Map<String, String>", "declarator": "headers", "var_name": "headers"}, {"original_string": "private Optional<SpecificDatumReader<AvroFlumeEvent>> reader = Optional.absent();", "modifier": "private", "type": "Optional<SpecificDatumReader<AvroFlumeEvent>>", "declarator": "reader = Optional.absent()", "var_name": "reader"}, {"original_string": "private BinaryDecoder decoder = null;", "modifier": "private", "type": "BinaryDecoder", "declarator": "decoder = null", "var_name": "decoder"}, {"original_string": "private boolean useAvroEventFormat;", "modifier": "private", "type": "boolean", "declarator": "useAvroEventFormat", "var_name": "useAvroEventFormat"}, {"original_string": "private int batchUpperLimit;", "modifier": "private", "type": "int", "declarator": "batchUpperLimit", "var_name": "batchUpperLimit"}, {"original_string": "private int maxBatchDurationMillis;", "modifier": "private", "type": "int", "declarator": "maxBatchDurationMillis", "var_name": "maxBatchDurationMillis"}, {"original_string": "private Subscriber subscriber;", "modifier": "private", "type": "Subscriber", "declarator": "subscriber", "var_name": "subscriber"}, {"original_string": "private String zookeeperConnect;", "modifier": "private", "type": "String", "declarator": "zookeeperConnect", "var_name": "zookeeperConnect"}, {"original_string": "private String bootstrapServers;", "modifier": "private", "type": "String", "declarator": "bootstrapServers", "var_name": "bootstrapServers"}, {"original_string": "private String groupId = DEFAULT_GROUP_ID;", "modifier": "private", "type": "String", "declarator": "groupId = DEFAULT_GROUP_ID", "var_name": "groupId"}, {"original_string": "@Deprecated\n  private boolean migrateZookeeperOffsets = DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS;", "modifier": "@Deprecated\n  private", "type": "boolean", "declarator": "migrateZookeeperOffsets = DEFAULT_MIGRATE_ZOOKEEPER_OFFSETS", "var_name": "migrateZookeeperOffsets"}, {"original_string": "private String topicHeader = null;", "modifier": "private", "type": "String", "declarator": "topicHeader = null", "var_name": "topicHeader"}, {"original_string": "private boolean setTopicHeader;", "modifier": "private", "type": "boolean", "declarator": "setTopicHeader", "var_name": "setTopicHeader"}], "methods": [{"identifier": "getBatchSize", "parameters": "()", "modifiers": "@Override public", "return": "long", "signature": "long getBatchSize()", "full_signature": "@Override public long getBatchSize()", "class_method_signature": "KafkaSource.getBatchSize()", "testcase": false, "constructor": false}, {"identifier": "doProcess", "parameters": "()", "modifiers": "@Override protected", "return": "Status", "signature": "Status doProcess()", "full_signature": "@Override protected Status doProcess()", "class_method_signature": "KafkaSource.doProcess()", "testcase": false, "constructor": false}, {"identifier": "doConfigure", "parameters": "(Context context)", "modifiers": "@Override protected", "return": "void", "signature": "void doConfigure(Context context)", "full_signature": "@Override protected void doConfigure(Context context)", "class_method_signature": "KafkaSource.doConfigure(Context context)", "testcase": false, "constructor": false}, {"identifier": "translateOldProperties", "parameters": "(Context ctx)", "modifiers": "private", "return": "void", "signature": "void translateOldProperties(Context ctx)", "full_signature": "private void translateOldProperties(Context ctx)", "class_method_signature": "KafkaSource.translateOldProperties(Context ctx)", "testcase": false, "constructor": false}, {"identifier": "setConsumerProps", "parameters": "(Context ctx)", "modifiers": "private", "return": "void", "signature": "void setConsumerProps(Context ctx)", "full_signature": "private void setConsumerProps(Context ctx)", "class_method_signature": "KafkaSource.setConsumerProps(Context ctx)", "testcase": false, "constructor": false}, {"identifier": "lookupBootstrap", "parameters": "(String zookeeperConnect, SecurityProtocol securityProtocol)", "modifiers": "private", "return": "String", "signature": "String lookupBootstrap(String zookeeperConnect, SecurityProtocol securityProtocol)", "full_signature": "private String lookupBootstrap(String zookeeperConnect, SecurityProtocol securityProtocol)", "class_method_signature": "KafkaSource.lookupBootstrap(String zookeeperConnect, SecurityProtocol securityProtocol)", "testcase": false, "constructor": false}, {"identifier": "getBootstrapServers", "parameters": "()", "modifiers": "@VisibleForTesting", "return": "String", "signature": "String getBootstrapServers()", "full_signature": "@VisibleForTesting String getBootstrapServers()", "class_method_signature": "KafkaSource.getBootstrapServers()", "testcase": false, "constructor": false}, {"identifier": "getConsumerProps", "parameters": "()", "modifiers": "", "return": "Properties", "signature": "Properties getConsumerProps()", "full_signature": " Properties getConsumerProps()", "class_method_signature": "KafkaSource.getConsumerProps()", "testcase": false, "constructor": false}, {"identifier": "toStringMap", "parameters": "(Map<CharSequence, CharSequence> charSeqMap)", "modifiers": "private static", "return": "Map<String, String>", "signature": "Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)", "full_signature": "private static Map<String, String> toStringMap(Map<CharSequence, CharSequence> charSeqMap)", "class_method_signature": "KafkaSource.toStringMap(Map<CharSequence, CharSequence> charSeqMap)", "testcase": false, "constructor": false}, {"identifier": "getSubscriber", "parameters": "()", "modifiers": "", "return": "Subscriber<T>", "signature": "Subscriber<T> getSubscriber()", "full_signature": " Subscriber<T> getSubscriber()", "class_method_signature": "KafkaSource.getSubscriber()", "testcase": false, "constructor": false}, {"identifier": "doStart", "parameters": "()", "modifiers": "@Override protected", "return": "void", "signature": "void doStart()", "full_signature": "@Override protected void doStart()", "class_method_signature": "KafkaSource.doStart()", "testcase": false, "constructor": false}, {"identifier": "doStop", "parameters": "()", "modifiers": "@Override protected", "return": "void", "signature": "void doStop()", "full_signature": "@Override protected void doStop()", "class_method_signature": "KafkaSource.doStop()", "testcase": false, "constructor": false}, {"identifier": "migrateOffsets", "parameters": "(String topicStr)", "modifiers": "private", "return": "void", "signature": "void migrateOffsets(String topicStr)", "full_signature": "private void migrateOffsets(String topicStr)", "class_method_signature": "KafkaSource.migrateOffsets(String topicStr)", "testcase": false, "constructor": false}, {"identifier": "getKafkaOffsets", "parameters": "(\n      KafkaConsumer<String, byte[]> client, String topicStr)", "modifiers": "private", "return": "Map<TopicPartition, OffsetAndMetadata>", "signature": "Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(\n      KafkaConsumer<String, byte[]> client, String topicStr)", "full_signature": "private Map<TopicPartition, OffsetAndMetadata> getKafkaOffsets(\n      KafkaConsumer<String, byte[]> client, String topicStr)", "class_method_signature": "KafkaSource.getKafkaOffsets(\n      KafkaConsumer<String, byte[]> client, String topicStr)", "testcase": false, "constructor": false}, {"identifier": "getZookeeperOffsets", "parameters": "(\n          KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)", "modifiers": "private", "return": "Map<TopicPartition, OffsetAndMetadata>", "signature": "Map<TopicPartition, OffsetAndMetadata> getZookeeperOffsets(\n          KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)", "full_signature": "private Map<TopicPartition, OffsetAndMetadata> getZookeeperOffsets(\n          KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)", "class_method_signature": "KafkaSource.getZookeeperOffsets(\n          KafkaZkClient zkClient, KafkaConsumer<String, byte[]> consumer, String topicStr)", "testcase": false, "constructor": false}], "file": "flume-ng-sources/flume-kafka-source/src/main/java/org/apache/flume/source/kafka/KafkaSource.java"}, "focal_method": {"identifier": "doProcess", "parameters": "()", "modifiers": "@Override protected", "return": "Status", "body": "@Override\n  protected Status doProcess() throws EventDeliveryException {\n    final String batchUUID = UUID.randomUUID().toString();\n    String kafkaKey;\n    Event event;\n    byte[] eventBody;\n\n    try {\n      // prepare time variables for new batch\n      final long nanoBatchStartTime = System.nanoTime();\n      final long batchStartTime = System.currentTimeMillis();\n      final long maxBatchEndTime = System.currentTimeMillis() + maxBatchDurationMillis;\n\n      while (eventList.size() < batchUpperLimit &&\n              System.currentTimeMillis() < maxBatchEndTime) {\n\n        if (it == null || !it.hasNext()) {\n          // Obtaining new records\n          // Poll time is remainder time for current batch.\n          long durMs = Math.max(0L, maxBatchEndTime - System.currentTimeMillis());\n          Duration duration = Duration.ofMillis(durMs);\n          ConsumerRecords<String, byte[]> records = consumer.poll(duration);\n          it = records.iterator();\n\n          // this flag is set to true in a callback when some partitions are revoked.\n          // If there are any records we commit them.\n          if (rebalanceFlag.compareAndSet(true, false)) {\n            break;\n          }\n          // check records after poll\n          if (!it.hasNext()) {\n            counter.incrementKafkaEmptyCount();\n            log.debug(\"Returning with backoff. No more data to read\");\n            // batch time exceeded\n            break;\n          }\n        }\n\n        // get next message\n        ConsumerRecord<String, byte[]> message = it.next();\n        kafkaKey = message.key();\n\n        if (useAvroEventFormat) {\n          //Assume the event is in Avro format using the AvroFlumeEvent schema\n          //Will need to catch the exception if it is not\n          ByteArrayInputStream in =\n                  new ByteArrayInputStream(message.value());\n          decoder = DecoderFactory.get().directBinaryDecoder(in, decoder);\n          if (!reader.isPresent()) {\n            reader = Optional.of(\n                    new SpecificDatumReader<AvroFlumeEvent>(AvroFlumeEvent.class));\n          }\n          //This may throw an exception but it will be caught by the\n          //exception handler below and logged at error\n          AvroFlumeEvent avroevent = reader.get().read(null, decoder);\n\n          eventBody = avroevent.getBody().array();\n          headers = toStringMap(avroevent.getHeaders());\n        } else {\n          eventBody = message.value();\n          headers.clear();\n          headers = new HashMap<String, String>(4);\n        }\n\n        // Add headers to event (timestamp, topic, partition, key) only if they don't exist\n        if (!headers.containsKey(KafkaSourceConstants.TIMESTAMP_HEADER)) {\n          headers.put(KafkaSourceConstants.TIMESTAMP_HEADER,\n              String.valueOf(System.currentTimeMillis()));\n        }\n        // Only set the topic header if setTopicHeader and it isn't already populated\n        if (setTopicHeader && !headers.containsKey(topicHeader)) {\n          headers.put(topicHeader, message.topic());\n        }\n        if (!headers.containsKey(KafkaSourceConstants.PARTITION_HEADER)) {\n          headers.put(KafkaSourceConstants.PARTITION_HEADER,\n              String.valueOf(message.partition()));\n        }\n        if (!headers.containsKey(OFFSET_HEADER)) {\n          headers.put(OFFSET_HEADER,\n              String.valueOf(message.offset()));\n        }\n\n        if (kafkaKey != null) {\n          headers.put(KafkaSourceConstants.KEY_HEADER, kafkaKey);\n        }\n\n        if (log.isTraceEnabled()) {\n          if (LogPrivacyUtil.allowLogRawData()) {\n            log.trace(\"Topic: {} Partition: {} Message: {}\", new String[]{\n                message.topic(),\n                String.valueOf(message.partition()),\n                new String(eventBody)\n            });\n          } else {\n            log.trace(\"Topic: {} Partition: {} Message arrived.\",\n                message.topic(),\n                String.valueOf(message.partition()));\n          }\n        }\n\n        event = EventBuilder.withBody(eventBody, headers);\n        eventList.add(event);\n\n        if (log.isDebugEnabled()) {\n          log.debug(\"Waited: {} \", System.currentTimeMillis() - batchStartTime);\n          log.debug(\"Event #: {}\", eventList.size());\n        }\n\n        // For each partition store next offset that is going to be read.\n        tpAndOffsetMetadata.put(new TopicPartition(message.topic(), message.partition()),\n                new OffsetAndMetadata(message.offset() + 1, batchUUID));\n      }\n\n      if (eventList.size() > 0) {\n        counter.addToKafkaEventGetTimer((System.nanoTime() - nanoBatchStartTime) / (1000 * 1000));\n        counter.addToEventReceivedCount((long) eventList.size());\n        getChannelProcessor().processEventBatch(eventList);\n        counter.addToEventAcceptedCount(eventList.size());\n        if (log.isDebugEnabled()) {\n          log.debug(\"Wrote {} events to channel\", eventList.size());\n        }\n        eventList.clear();\n\n        if (!tpAndOffsetMetadata.isEmpty()) {\n          long commitStartTime = System.nanoTime();\n          consumer.commitSync(tpAndOffsetMetadata);\n          long commitEndTime = System.nanoTime();\n          counter.addToKafkaCommitTimer((commitEndTime - commitStartTime) / (1000 * 1000));\n          tpAndOffsetMetadata.clear();\n        }\n        return Status.READY;\n      }\n\n      return Status.BACKOFF;\n    } catch (Exception e) {\n      log.error(\"KafkaSource EXCEPTION, {}\", e);\n      counter.incrementEventReadOrChannelFail(e);\n      return Status.BACKOFF;\n    }\n  }", "signature": "Status doProcess()", "full_signature": "@Override protected Status doProcess()", "class_method_signature": "KafkaSource.doProcess()", "testcase": false, "constructor": false, "invocations": ["toString", "randomUUID", "nanoTime", "currentTimeMillis", "currentTimeMillis", "size", "currentTimeMillis", "hasNext", "max", "currentTimeMillis", "ofMillis", "poll", "iterator", "compareAndSet", "hasNext", "incrementKafkaEmptyCount", "debug", "next", "key", "value", "directBinaryDecoder", "get", "isPresent", "of", "read", "get", "array", "getBody", "toStringMap", "getHeaders", "value", "clear", "containsKey", "put", "valueOf", "currentTimeMillis", "containsKey", "put", "topic", "containsKey", "put", "valueOf", "partition", "containsKey", "put", "valueOf", "offset", "put", "isTraceEnabled", "allowLogRawData", "trace", "topic", "valueOf", "partition", "trace", "topic", "valueOf", "partition", "withBody", "add", "isDebugEnabled", "debug", "currentTimeMillis", "debug", "size", "put", "topic", "partition", "offset", "size", "addToKafkaEventGetTimer", "nanoTime", "addToEventReceivedCount", "size", "processEventBatch", "getChannelProcessor", "addToEventAcceptedCount", "size", "isDebugEnabled", "debug", "size", "clear", "isEmpty", "nanoTime", "commitSync", "nanoTime", "addToKafkaCommitTimer", "clear", "error", "incrementEventReadOrChannelFail"]}, "repository": {"repo_id": 2198510, "url": "https://github.com/apache/flume", "language": "Java", "is_fork": false, "fork_count": 1372, "stargazer_count": 1971, "size": 44377, "license": "licensed"}}