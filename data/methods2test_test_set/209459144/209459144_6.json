{"test_class": {"identifier": "SparkInterpreterTest", "superclass": "", "interfaces": "", "fields": [{"original_string": "private SparkInterpreter interpreter;", "modifier": "private", "type": "SparkInterpreter", "declarator": "interpreter", "var_name": "interpreter"}, {"original_string": "private volatile String output = \"\";", "modifier": "private volatile", "type": "String", "declarator": "output = \"\"", "var_name": "output"}], "file": "submarine-workbench/interpreter/spark-interpreter/src/test/java/org/apache/submarine/interpreter/SparkInterpreterTest.java"}, "test_case": {"identifier": "testSparkInterpreter", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void testSparkInterpreter() throws InterruptedException, InterpreterException {\n    Properties properties = new Properties();\n    properties.setProperty(\"spark.master\", \"local\");\n    properties.setProperty(\"spark.app.name\", \"test\");\n    properties.setProperty(\"submarine.spark.scala.color\", \"false\");\n    properties.setProperty(\"submarine.spark.test\", \"true\");\n    properties.setProperty(\"submarine.spark.uiWebUrl\", \"fake_spark_weburl\");\n    // disable color output for easy testing\n    properties.setProperty(\"submarine.spark.deprecatedMsg.show\", \"false\");\n\n    interpreter = new SparkInterpreter(properties);\n\n    interpreter.open();\n\n    InterpreterResult result = interpreter.interpret(\"val a=\\\"hello world\\\"\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertEquals(\"a: String = hello world\\n\", result.message().get(0).getData());\n\n    result = interpreter.interpret(\"print(a)\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertEquals(\"hello world\", result.message().get(0).getData());\n\n    // java stdout\n    result = interpreter.interpret(\"System.out.print(a)\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertEquals(\"hello world\", result.message().get(0).getData());\n\n    // incomplete\n    result = interpreter.interpret(\"println(a\");\n    assertEquals(InterpreterResult.Code.INCOMPLETE, result.code());\n\n    // syntax error\n    result = interpreter.interpret(\"println(b)\");\n    assertEquals(InterpreterResult.Code.ERROR, result.code());\n    assertTrue(result.message().get(0).getData().contains(\"not found: value b\"));\n\n    //multiple line\n    result = interpreter.interpret(\"\\\"123\\\".\\ntoInt\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // single line comment\n    result = interpreter.interpret(\"print(\\\"hello world\\\")/*comment here*/\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertEquals(\"hello world\", result.message().get(0).getData());\n\n    result = interpreter.interpret(\"/*comment here*/\\nprint(\\\"hello world\\\")\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // multiple line comment\n    result = interpreter.interpret(\"/*line 1 \\n line 2*/\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // test function\n    result = interpreter.interpret(\"def add(x:Int, y:Int)\\n{ return x+y }\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    result = interpreter.interpret(\"print(add(1,2))\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    result = interpreter.interpret(\"/*line 1 \\n line 2*/print(\\\"hello world\\\")\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // Companion object with case class\n    result = interpreter.interpret(\n            \"import scala.math._\\n\" +\n                  \"object Circle {\\n\" +\n                  \"private def calculateArea(radius: Double): Double = Pi * pow(radius, 2.0)\\n\" +\n                  \"}\\n\" +\n                  \"case class Circle(radius: Double) {\\n\" +\n                  \"  import Circle._\\n\" +\n                  \"  def area: Double = calculateArea(radius)\\n\" +\n                  \"}\\n\" +\n                  \"\\n\" +\n                  \"val circle1 = new Circle(5.0)\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // class extend\n    result = interpreter.interpret(\"import java.util.ArrayList\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    result = interpreter.interpret(\"sc\\n.range(1, 10)\\n.sum\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertTrue(result.message().get(0).getData().contains(\"45\"));\n    result = interpreter.interpret(\"sc\\n.range(1, 10)\\n.sum\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertTrue(result.message().get(0).getData().contains(\"45\"));\n    result = interpreter.interpret(\"val bankText = sc.textFile(\\\"bank.csv\\\")\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    result = interpreter.interpret(\n            \"case class Bank(age:Integer, job:String, marital : String, edu : String, balance : Integer)\\n\" +\n                  \"val bank = bankText.map(s=>s.split(\\\";\\\")).filter(s => s(0)!=\\\"\\\\\\\"age\\\\\\\"\\\").map(\\n\" +\n                  \"    s => Bank(s(0).toInt, \\n\" +\n                  \"            s(1).replaceAll(\\\"\\\\\\\"\\\", \\\"\\\"),\\n\" +\n                  \"            s(2).replaceAll(\\\"\\\\\\\"\\\", \\\"\\\"),\\n\" +\n                  \"            s(3).replaceAll(\\\"\\\\\\\"\\\", \\\"\\\"),\\n\" +\n                  \"            s(5).replaceAll(\\\"\\\\\\\"\\\", \\\"\\\").toInt\\n\" +\n                  \"        )\\n\" +\n                  \").toDF()\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // spark version\n    result = interpreter.interpret(\"sc.version\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // spark sql test\n    String version = result.message().get(0).getData().trim();\n    if (version.contains(\"String = 1.\")) {\n      result = interpreter.interpret(\"sqlContext\");\n      assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n      result = interpreter.interpret(\n              \"val df = sqlContext.createDataFrame(Seq((1,\\\"a\\\"),(2, null)))\\n\" +\n                    \"df.show()\");\n      assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n      assertTrue(result.message().get(0).getData().contains(\n              \"+---+----+\\n\" +\n              \"| _1|  _2|\\n\" +\n              \"+---+----+\\n\" +\n              \"|  1|   a|\\n\" +\n              \"|  2|null|\\n\" +\n              \"+---+----+\"));\n    } else if (version.contains(\"String = 2.\")) {\n      result = interpreter.interpret(\"spark\");\n      assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n      result = interpreter.interpret(\n              \"val df = spark.createDataFrame(Seq((1,\\\"a\\\"),(2, null)))\\n\" +\n                      \"df.show()\");\n      assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n      assertTrue(result.message().get(0).getData().contains(\n              \"+---+----+\\n\" +\n              \"| _1|  _2|\\n\" +\n              \"+---+----+\\n\" +\n              \"|  1|   a|\\n\" +\n              \"|  2|null|\\n\" +\n              \"+---+----+\"));\n    }\n\n    // submarineContext\n    result = interpreter.interpret(\"z.show(df)\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n    assertEquals(InterpreterResult.Type.TABLE, result.message().get(0).getType());\n    assertEquals(\"_1\\t_2\\n1\\ta\\n2\\tnull\\n\", result.message().get(0).getData());\n\n    result = interpreter.interpret(\"z.input(\\\"name\\\", \\\"default_name\\\")\");\n    assertEquals(InterpreterResult.Code.SUCCESS, result.code());\n\n    // getProgress;\n    Thread interpretThread = new Thread(() -> {\n      try {\n        InterpreterResult result1 = interpreter.interpret(\n                \"val df = sc.parallelize(1 to 10, 5).foreach(e=>Thread.sleep(1000))\");\n        assertEquals(InterpreterResult.Code.SUCCESS, result1.code());\n      } catch (InterpreterException e) {\n        e.printStackTrace();\n        fail();\n      }\n    });\n    interpretThread.start();\n    boolean nonZeroProgress = false;\n    int progress = 0;\n    while (interpretThread.isAlive()) {\n      progress = interpreter.getProgress();\n      assertTrue(progress >= 0);\n      if (progress != 0 && progress != 100) {\n        nonZeroProgress = true;\n      }\n      Thread.sleep(100);\n    }\n    assertTrue(nonZeroProgress);\n\n    interpretThread = new Thread(() -> {\n      try {\n        InterpreterResult result12 = interpreter.interpret(\n                \"val df = sc.parallelize(1 to 10, 2).foreach(e=>Thread.sleep(1000))\");\n        assertEquals(InterpreterResult.Code.ERROR, result12.code());\n        assertTrue(result12.message().get(0).getData().contains(\"cancelled\"));\n      } catch (org.apache.submarine.interpreter.InterpreterException e) {\n        e.printStackTrace();\n        fail();\n      }\n    });\n\n    interpretThread.start();\n    // sleep 1 second to wait for the spark job start\n    Thread.sleep(1000);\n    interpreter.cancel();\n    interpretThread.join();\n  }", "signature": "void testSparkInterpreter()", "full_signature": "@Test public void testSparkInterpreter()", "class_method_signature": "SparkInterpreterTest.testSparkInterpreter()", "testcase": true, "constructor": false, "invocations": ["setProperty", "setProperty", "setProperty", "setProperty", "setProperty", "setProperty", "open", "interpret", "assertEquals", "code", "assertEquals", "getData", "get", "message", "interpret", "assertEquals", "code", "assertEquals", "getData", "get", "message", "interpret", "assertEquals", "code", "assertEquals", "getData", "get", "message", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "assertEquals", "getData", "get", "message", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "trim", "getData", "get", "message", "contains", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "contains", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "interpret", "assertEquals", "code", "assertEquals", "getType", "get", "message", "assertEquals", "getData", "get", "message", "interpret", "assertEquals", "code", "interpret", "assertEquals", "code", "printStackTrace", "fail", "start", "isAlive", "getProgress", "assertTrue", "sleep", "assertTrue", "interpret", "assertEquals", "code", "assertTrue", "contains", "getData", "get", "message", "printStackTrace", "fail", "start", "sleep", "cancel", "join"]}, "focal_class": {"identifier": "SparkInterpreter", "superclass": "extends AbstractInterpreter", "interfaces": "", "fields": [{"original_string": "private static final Logger LOG = LoggerFactory.getLogger(SparkInterpreter.class);", "modifier": "private static final", "type": "Logger", "declarator": "LOG = LoggerFactory.getLogger(SparkInterpreter.class)", "var_name": "LOG"}], "methods": [{"identifier": "SparkInterpreter", "parameters": "(Properties properties)", "modifiers": "public", "return": "", "signature": " SparkInterpreter(Properties properties)", "full_signature": "public  SparkInterpreter(Properties properties)", "class_method_signature": "SparkInterpreter.SparkInterpreter(Properties properties)", "testcase": false, "constructor": true}, {"identifier": "SparkInterpreter", "parameters": "()", "modifiers": "public", "return": "", "signature": " SparkInterpreter()", "full_signature": "public  SparkInterpreter()", "class_method_signature": "SparkInterpreter.SparkInterpreter()", "testcase": false, "constructor": true}, {"identifier": "test", "parameters": "()", "modifiers": "@Override public", "return": "boolean", "signature": "boolean test()", "full_signature": "@Override public boolean test()", "class_method_signature": "SparkInterpreter.test()", "testcase": false, "constructor": false}, {"identifier": "getSparkContext", "parameters": "()", "modifiers": "public", "return": "SparkContext", "signature": "SparkContext getSparkContext()", "full_signature": "public SparkContext getSparkContext()", "class_method_signature": "SparkInterpreter.getSparkContext()", "testcase": false, "constructor": false}, {"identifier": "setSchedulerPool", "parameters": "(String pool)", "modifiers": "public", "return": "void", "signature": "void setSchedulerPool(String pool)", "full_signature": "public void setSchedulerPool(String pool)", "class_method_signature": "SparkInterpreter.setSchedulerPool(String pool)", "testcase": false, "constructor": false}, {"identifier": "mergeZeppelinInterpreterProperties", "parameters": "(Properties properties)", "modifiers": "@Override protected", "return": "Properties", "signature": "Properties mergeZeppelinInterpreterProperties(Properties properties)", "full_signature": "@Override protected Properties mergeZeppelinInterpreterProperties(Properties properties)", "class_method_signature": "SparkInterpreter.mergeZeppelinInterpreterProperties(Properties properties)", "testcase": false, "constructor": false}], "file": "submarine-workbench/interpreter/spark-interpreter/src/main/java/org/apache/submarine/interpreter/SparkInterpreter.java"}, "focal_method": {"identifier": "SparkInterpreter", "parameters": "(Properties properties)", "modifiers": "public", "return": "", "body": "public SparkInterpreter(Properties properties) {\n    properties = mergeZeppelinInterpreterProperties(properties);\n    this.zeppelinInterpreter = new org.apache.zeppelin.spark.SparkInterpreter(properties);\n    this.setInterpreterGroup(new InterpreterGroup());\n  }", "signature": " SparkInterpreter(Properties properties)", "full_signature": "public  SparkInterpreter(Properties properties)", "class_method_signature": "SparkInterpreter.SparkInterpreter(Properties properties)", "testcase": false, "constructor": true, "invocations": []}, "repository": {"repo_id": 209459144, "url": "https://github.com/apache/submarine", "stars": 185, "created": "9/19/2019 4:00:17 AM +00:00", "updates": "2020-01-27T15:27:05+00:00", "fork": "False", "license": "licensed"}}