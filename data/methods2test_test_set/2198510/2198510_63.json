{"test_class": {"identifier": "TestHDFSEventSink", "superclass": "", "interfaces": "", "fields": [{"original_string": "private HDFSEventSink sink;", "modifier": "private", "type": "HDFSEventSink", "declarator": "sink", "var_name": "sink"}, {"original_string": "private String testPath;", "modifier": "private", "type": "String", "declarator": "testPath", "var_name": "testPath"}, {"original_string": "private static final Logger LOG = LoggerFactory\n      .getLogger(HDFSEventSink.class);", "modifier": "private static final", "type": "Logger", "declarator": "LOG = LoggerFactory\n      .getLogger(HDFSEventSink.class)", "var_name": "LOG"}], "file": "flume-ng-sinks/flume-hdfs-sink/src/test/java/org/apache/flume/sink/hdfs/TestHDFSEventSink.java"}, "test_case": {"identifier": "testKerbFileAccess", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void testKerbFileAccess() throws InterruptedException,\n      LifecycleException, EventDeliveryException, IOException {\n    LOG.debug(\"Starting testKerbFileAccess() ...\");\n    final String fileName = \"FlumeData\";\n    final long rollCount = 5;\n    final long batchSize = 2;\n    String newPath = testPath + \"/singleBucket\";\n    String kerbConfPrincipal = \"user1/localhost@EXAMPLE.COM\";\n    String kerbKeytab = \"/usr/lib/flume/nonexistkeytabfile\";\n\n    //turn security on\n    Configuration conf = new Configuration();\n    conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION,\n        \"kerberos\");\n    UserGroupInformation.setConfiguration(conf);\n\n    Context context = new Context();\n    context.put(\"hdfs.path\", newPath);\n    context.put(\"hdfs.filePrefix\", fileName);\n    context.put(\"hdfs.rollCount\", String.valueOf(rollCount));\n    context.put(\"hdfs.batchSize\", String.valueOf(batchSize));\n    context.put(\"hdfs.kerberosPrincipal\", kerbConfPrincipal);\n    context.put(\"hdfs.kerberosKeytab\", kerbKeytab);\n\n    try {\n      Configurables.configure(sink, context);\n      Assert.fail(\"no exception thrown\");\n    } catch (IllegalArgumentException expected) {\n      Assert.assertTrue(expected.getMessage().contains(\n          \"Keytab is not a readable file\"));\n    } finally {\n      //turn security off\n      conf.set(CommonConfigurationKeys.HADOOP_SECURITY_AUTHENTICATION,\n          \"simple\");\n      UserGroupInformation.setConfiguration(conf);\n    }\n  }", "signature": "void testKerbFileAccess()", "full_signature": "@Test public void testKerbFileAccess()", "class_method_signature": "TestHDFSEventSink.testKerbFileAccess()", "testcase": true, "constructor": false, "invocations": ["debug", "set", "setConfiguration", "put", "put", "put", "valueOf", "put", "valueOf", "put", "put", "configure", "fail", "assertTrue", "contains", "getMessage", "set", "setConfiguration"]}, "focal_class": {"identifier": "HDFSEventSink", "superclass": "extends AbstractSink", "interfaces": "implements Configurable, BatchSizeSupported", "fields": [{"original_string": "private static final Logger LOG = LoggerFactory.getLogger(HDFSEventSink.class);", "modifier": "private static final", "type": "Logger", "declarator": "LOG = LoggerFactory.getLogger(HDFSEventSink.class)", "var_name": "LOG"}, {"original_string": "private static String DIRECTORY_DELIMITER = System.getProperty(\"file.separator\");", "modifier": "private static", "type": "String", "declarator": "DIRECTORY_DELIMITER = System.getProperty(\"file.separator\")", "var_name": "DIRECTORY_DELIMITER"}, {"original_string": "private static final long defaultRollInterval = 30;", "modifier": "private static final", "type": "long", "declarator": "defaultRollInterval = 30", "var_name": "defaultRollInterval"}, {"original_string": "private static final long defaultRollSize = 1024;", "modifier": "private static final", "type": "long", "declarator": "defaultRollSize = 1024", "var_name": "defaultRollSize"}, {"original_string": "private static final long defaultRollCount = 10;", "modifier": "private static final", "type": "long", "declarator": "defaultRollCount = 10", "var_name": "defaultRollCount"}, {"original_string": "private static final String defaultFileName = \"FlumeData\";", "modifier": "private static final", "type": "String", "declarator": "defaultFileName = \"FlumeData\"", "var_name": "defaultFileName"}, {"original_string": "private static final String defaultSuffix = \"\";", "modifier": "private static final", "type": "String", "declarator": "defaultSuffix = \"\"", "var_name": "defaultSuffix"}, {"original_string": "private static final String defaultInUsePrefix = \"\";", "modifier": "private static final", "type": "String", "declarator": "defaultInUsePrefix = \"\"", "var_name": "defaultInUsePrefix"}, {"original_string": "private static final String defaultInUseSuffix = \".tmp\";", "modifier": "private static final", "type": "String", "declarator": "defaultInUseSuffix = \".tmp\"", "var_name": "defaultInUseSuffix"}, {"original_string": "private static final long defaultBatchSize = 100;", "modifier": "private static final", "type": "long", "declarator": "defaultBatchSize = 100", "var_name": "defaultBatchSize"}, {"original_string": "private static final String defaultFileType = HDFSWriterFactory.SequenceFileType;", "modifier": "private static final", "type": "String", "declarator": "defaultFileType = HDFSWriterFactory.SequenceFileType", "var_name": "defaultFileType"}, {"original_string": "private static final int defaultMaxOpenFiles = 5000;", "modifier": "private static final", "type": "int", "declarator": "defaultMaxOpenFiles = 5000", "var_name": "defaultMaxOpenFiles"}, {"original_string": "private static final long defaultRetryInterval = 180;", "modifier": "private static final", "type": "long", "declarator": "defaultRetryInterval = 180", "var_name": "defaultRetryInterval"}, {"original_string": "private static final int defaultTryCount = Integer.MAX_VALUE;", "modifier": "private static final", "type": "int", "declarator": "defaultTryCount = Integer.MAX_VALUE", "var_name": "defaultTryCount"}, {"original_string": "public static final String IN_USE_SUFFIX_PARAM_NAME = \"hdfs.inUseSuffix\";", "modifier": "public static final", "type": "String", "declarator": "IN_USE_SUFFIX_PARAM_NAME = \"hdfs.inUseSuffix\"", "var_name": "IN_USE_SUFFIX_PARAM_NAME"}, {"original_string": "private static final long defaultCallTimeout = 30000;", "modifier": "private static final", "type": "long", "declarator": "defaultCallTimeout = 30000", "var_name": "defaultCallTimeout"}, {"original_string": "private static final int defaultThreadPoolSize = 10;", "modifier": "private static final", "type": "int", "declarator": "defaultThreadPoolSize = 10", "var_name": "defaultThreadPoolSize"}, {"original_string": "private static final int defaultRollTimerPoolSize = 1;", "modifier": "private static final", "type": "int", "declarator": "defaultRollTimerPoolSize = 1", "var_name": "defaultRollTimerPoolSize"}, {"original_string": "private final HDFSWriterFactory writerFactory;", "modifier": "private final", "type": "HDFSWriterFactory", "declarator": "writerFactory", "var_name": "writerFactory"}, {"original_string": "private WriterLinkedHashMap sfWriters;", "modifier": "private", "type": "WriterLinkedHashMap", "declarator": "sfWriters", "var_name": "sfWriters"}, {"original_string": "private long rollInterval;", "modifier": "private", "type": "long", "declarator": "rollInterval", "var_name": "rollInterval"}, {"original_string": "private long rollSize;", "modifier": "private", "type": "long", "declarator": "rollSize", "var_name": "rollSize"}, {"original_string": "private long rollCount;", "modifier": "private", "type": "long", "declarator": "rollCount", "var_name": "rollCount"}, {"original_string": "private long batchSize;", "modifier": "private", "type": "long", "declarator": "batchSize", "var_name": "batchSize"}, {"original_string": "private int threadsPoolSize;", "modifier": "private", "type": "int", "declarator": "threadsPoolSize", "var_name": "threadsPoolSize"}, {"original_string": "private int rollTimerPoolSize;", "modifier": "private", "type": "int", "declarator": "rollTimerPoolSize", "var_name": "rollTimerPoolSize"}, {"original_string": "private CompressionCodec codeC;", "modifier": "private", "type": "CompressionCodec", "declarator": "codeC", "var_name": "codeC"}, {"original_string": "private CompressionType compType;", "modifier": "private", "type": "CompressionType", "declarator": "compType", "var_name": "compType"}, {"original_string": "private String fileType;", "modifier": "private", "type": "String", "declarator": "fileType", "var_name": "fileType"}, {"original_string": "private String filePath;", "modifier": "private", "type": "String", "declarator": "filePath", "var_name": "filePath"}, {"original_string": "private String fileName;", "modifier": "private", "type": "String", "declarator": "fileName", "var_name": "fileName"}, {"original_string": "private String suffix;", "modifier": "private", "type": "String", "declarator": "suffix", "var_name": "suffix"}, {"original_string": "private String inUsePrefix;", "modifier": "private", "type": "String", "declarator": "inUsePrefix", "var_name": "inUsePrefix"}, {"original_string": "private String inUseSuffix;", "modifier": "private", "type": "String", "declarator": "inUseSuffix", "var_name": "inUseSuffix"}, {"original_string": "private TimeZone timeZone;", "modifier": "private", "type": "TimeZone", "declarator": "timeZone", "var_name": "timeZone"}, {"original_string": "private int maxOpenFiles;", "modifier": "private", "type": "int", "declarator": "maxOpenFiles", "var_name": "maxOpenFiles"}, {"original_string": "private ExecutorService callTimeoutPool;", "modifier": "private", "type": "ExecutorService", "declarator": "callTimeoutPool", "var_name": "callTimeoutPool"}, {"original_string": "private ScheduledExecutorService timedRollerPool;", "modifier": "private", "type": "ScheduledExecutorService", "declarator": "timedRollerPool", "var_name": "timedRollerPool"}, {"original_string": "private boolean needRounding = false;", "modifier": "private", "type": "boolean", "declarator": "needRounding = false", "var_name": "needRounding"}, {"original_string": "private int roundUnit = Calendar.SECOND;", "modifier": "private", "type": "int", "declarator": "roundUnit = Calendar.SECOND", "var_name": "roundUnit"}, {"original_string": "private int roundValue = 1;", "modifier": "private", "type": "int", "declarator": "roundValue = 1", "var_name": "roundValue"}, {"original_string": "private boolean useLocalTime = false;", "modifier": "private", "type": "boolean", "declarator": "useLocalTime = false", "var_name": "useLocalTime"}, {"original_string": "private long callTimeout;", "modifier": "private", "type": "long", "declarator": "callTimeout", "var_name": "callTimeout"}, {"original_string": "private Context context;", "modifier": "private", "type": "Context", "declarator": "context", "var_name": "context"}, {"original_string": "private SinkCounter sinkCounter;", "modifier": "private", "type": "SinkCounter", "declarator": "sinkCounter", "var_name": "sinkCounter"}, {"original_string": "private volatile int idleTimeout;", "modifier": "private volatile", "type": "int", "declarator": "idleTimeout", "var_name": "idleTimeout"}, {"original_string": "private Clock clock;", "modifier": "private", "type": "Clock", "declarator": "clock", "var_name": "clock"}, {"original_string": "private FileSystem mockFs;", "modifier": "private", "type": "FileSystem", "declarator": "mockFs", "var_name": "mockFs"}, {"original_string": "private HDFSWriter mockWriter;", "modifier": "private", "type": "HDFSWriter", "declarator": "mockWriter", "var_name": "mockWriter"}, {"original_string": "private final Object sfWritersLock = new Object();", "modifier": "private final", "type": "Object", "declarator": "sfWritersLock = new Object()", "var_name": "sfWritersLock"}, {"original_string": "private long retryInterval;", "modifier": "private", "type": "long", "declarator": "retryInterval", "var_name": "retryInterval"}, {"original_string": "private int tryCount;", "modifier": "private", "type": "int", "declarator": "tryCount", "var_name": "tryCount"}, {"original_string": "private PrivilegedExecutor privExecutor;", "modifier": "private", "type": "PrivilegedExecutor", "declarator": "privExecutor", "var_name": "privExecutor"}], "methods": [{"identifier": "HDFSEventSink", "parameters": "()", "modifiers": "public", "return": "", "signature": " HDFSEventSink()", "full_signature": "public  HDFSEventSink()", "class_method_signature": "HDFSEventSink.HDFSEventSink()", "testcase": false, "constructor": true}, {"identifier": "HDFSEventSink", "parameters": "(HDFSWriterFactory writerFactory)", "modifiers": "public", "return": "", "signature": " HDFSEventSink(HDFSWriterFactory writerFactory)", "full_signature": "public  HDFSEventSink(HDFSWriterFactory writerFactory)", "class_method_signature": "HDFSEventSink.HDFSEventSink(HDFSWriterFactory writerFactory)", "testcase": false, "constructor": true}, {"identifier": "getSfWriters", "parameters": "()", "modifiers": "@VisibleForTesting", "return": "Map<String, BucketWriter>", "signature": "Map<String, BucketWriter> getSfWriters()", "full_signature": "@VisibleForTesting Map<String, BucketWriter> getSfWriters()", "class_method_signature": "HDFSEventSink.getSfWriters()", "testcase": false, "constructor": false}, {"identifier": "configure", "parameters": "(Context context)", "modifiers": "@Override public", "return": "void", "signature": "void configure(Context context)", "full_signature": "@Override public void configure(Context context)", "class_method_signature": "HDFSEventSink.configure(Context context)", "testcase": false, "constructor": false}, {"identifier": "codecMatches", "parameters": "(Class<? extends CompressionCodec> cls, String codecName)", "modifiers": "private static", "return": "boolean", "signature": "boolean codecMatches(Class<? extends CompressionCodec> cls, String codecName)", "full_signature": "private static boolean codecMatches(Class<? extends CompressionCodec> cls, String codecName)", "class_method_signature": "HDFSEventSink.codecMatches(Class<? extends CompressionCodec> cls, String codecName)", "testcase": false, "constructor": false}, {"identifier": "getCodec", "parameters": "(String codecName)", "modifiers": "@VisibleForTesting static", "return": "CompressionCodec", "signature": "CompressionCodec getCodec(String codecName)", "full_signature": "@VisibleForTesting static CompressionCodec getCodec(String codecName)", "class_method_signature": "HDFSEventSink.getCodec(String codecName)", "testcase": false, "constructor": false}, {"identifier": "process", "parameters": "()", "modifiers": "public", "return": "Status", "signature": "Status process()", "full_signature": "public Status process()", "class_method_signature": "HDFSEventSink.process()", "testcase": false, "constructor": false}, {"identifier": "initializeBucketWriter", "parameters": "(String realPath,\n      String realName, String lookupPath, HDFSWriter hdfsWriter,\n      WriterCallback closeCallback)", "modifiers": "@VisibleForTesting", "return": "BucketWriter", "signature": "BucketWriter initializeBucketWriter(String realPath,\n      String realName, String lookupPath, HDFSWriter hdfsWriter,\n      WriterCallback closeCallback)", "full_signature": "@VisibleForTesting BucketWriter initializeBucketWriter(String realPath,\n      String realName, String lookupPath, HDFSWriter hdfsWriter,\n      WriterCallback closeCallback)", "class_method_signature": "HDFSEventSink.initializeBucketWriter(String realPath,\n      String realName, String lookupPath, HDFSWriter hdfsWriter,\n      WriterCallback closeCallback)", "testcase": false, "constructor": false}, {"identifier": "stop", "parameters": "()", "modifiers": "@Override public", "return": "void", "signature": "void stop()", "full_signature": "@Override public void stop()", "class_method_signature": "HDFSEventSink.stop()", "testcase": false, "constructor": false}, {"identifier": "start", "parameters": "()", "modifiers": "@Override public", "return": "void", "signature": "void start()", "full_signature": "@Override public void start()", "class_method_signature": "HDFSEventSink.start()", "testcase": false, "constructor": false}, {"identifier": "toString", "parameters": "()", "modifiers": "@Override public", "return": "String", "signature": "String toString()", "full_signature": "@Override public String toString()", "class_method_signature": "HDFSEventSink.toString()", "testcase": false, "constructor": false}, {"identifier": "setBucketClock", "parameters": "(Clock clock)", "modifiers": "@VisibleForTesting", "return": "void", "signature": "void setBucketClock(Clock clock)", "full_signature": "@VisibleForTesting void setBucketClock(Clock clock)", "class_method_signature": "HDFSEventSink.setBucketClock(Clock clock)", "testcase": false, "constructor": false}, {"identifier": "setMockFs", "parameters": "(FileSystem mockFs)", "modifiers": "@VisibleForTesting", "return": "void", "signature": "void setMockFs(FileSystem mockFs)", "full_signature": "@VisibleForTesting void setMockFs(FileSystem mockFs)", "class_method_signature": "HDFSEventSink.setMockFs(FileSystem mockFs)", "testcase": false, "constructor": false}, {"identifier": "setMockWriter", "parameters": "(HDFSWriter writer)", "modifiers": "@VisibleForTesting", "return": "void", "signature": "void setMockWriter(HDFSWriter writer)", "full_signature": "@VisibleForTesting void setMockWriter(HDFSWriter writer)", "class_method_signature": "HDFSEventSink.setMockWriter(HDFSWriter writer)", "testcase": false, "constructor": false}, {"identifier": "getTryCount", "parameters": "()", "modifiers": "@VisibleForTesting", "return": "int", "signature": "int getTryCount()", "full_signature": "@VisibleForTesting int getTryCount()", "class_method_signature": "HDFSEventSink.getTryCount()", "testcase": false, "constructor": false}, {"identifier": "getBatchSize", "parameters": "()", "modifiers": "@Override public", "return": "long", "signature": "long getBatchSize()", "full_signature": "@Override public long getBatchSize()", "class_method_signature": "HDFSEventSink.getBatchSize()", "testcase": false, "constructor": false}], "file": "flume-ng-sinks/flume-hdfs-sink/src/main/java/org/apache/flume/sink/hdfs/HDFSEventSink.java"}, "focal_method": {"identifier": "configure", "parameters": "(Context context)", "modifiers": "@Override public", "return": "void", "body": "@Override\n  public void configure(Context context) {\n    this.context = context;\n\n    filePath = Preconditions.checkNotNull(\n        context.getString(\"hdfs.path\"), \"hdfs.path is required\");\n    fileName = context.getString(\"hdfs.filePrefix\", defaultFileName);\n    this.suffix = context.getString(\"hdfs.fileSuffix\", defaultSuffix);\n    inUsePrefix = context.getString(\"hdfs.inUsePrefix\", defaultInUsePrefix);\n    boolean emptyInUseSuffix = context.getBoolean(\"hdfs.emptyInUseSuffix\", false);\n    if (emptyInUseSuffix) {\n      inUseSuffix = \"\";\n      String tmpInUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME);\n      if (tmpInUseSuffix != null) {\n        LOG.warn(\"Ignoring parameter \" + IN_USE_SUFFIX_PARAM_NAME + \" for hdfs sink: \" + getName());\n      }\n    } else {\n      inUseSuffix = context.getString(IN_USE_SUFFIX_PARAM_NAME, defaultInUseSuffix);\n    }\n    String tzName = context.getString(\"hdfs.timeZone\");\n    timeZone = tzName == null ? null : TimeZone.getTimeZone(tzName);\n    rollInterval = context.getLong(\"hdfs.rollInterval\", defaultRollInterval);\n    rollSize = context.getLong(\"hdfs.rollSize\", defaultRollSize);\n    rollCount = context.getLong(\"hdfs.rollCount\", defaultRollCount);\n    batchSize = context.getLong(\"hdfs.batchSize\", defaultBatchSize);\n    idleTimeout = context.getInteger(\"hdfs.idleTimeout\", 0);\n    String codecName = context.getString(\"hdfs.codeC\");\n    fileType = context.getString(\"hdfs.fileType\", defaultFileType);\n    maxOpenFiles = context.getInteger(\"hdfs.maxOpenFiles\", defaultMaxOpenFiles);\n    callTimeout = context.getLong(\"hdfs.callTimeout\", defaultCallTimeout);\n    threadsPoolSize = context.getInteger(\"hdfs.threadsPoolSize\",\n        defaultThreadPoolSize);\n    rollTimerPoolSize = context.getInteger(\"hdfs.rollTimerPoolSize\",\n        defaultRollTimerPoolSize);\n    String kerbConfPrincipal = context.getString(\"hdfs.kerberosPrincipal\");\n    String kerbKeytab = context.getString(\"hdfs.kerberosKeytab\");\n    String proxyUser = context.getString(\"hdfs.proxyUser\");\n    tryCount = context.getInteger(\"hdfs.closeTries\", defaultTryCount);\n    if (tryCount <= 0) {\n      LOG.warn(\"Retry count value : \" + tryCount + \" is not \" +\n          \"valid. The sink will try to close the file until the file \" +\n          \"is eventually closed.\");\n      tryCount = defaultTryCount;\n    }\n    retryInterval = context.getLong(\"hdfs.retryInterval\", defaultRetryInterval);\n    if (retryInterval <= 0) {\n      LOG.warn(\"Retry Interval value: \" + retryInterval + \" is not \" +\n          \"valid. If the first close of a file fails, \" +\n          \"it may remain open and will not be renamed.\");\n      tryCount = 1;\n    }\n\n    Preconditions.checkArgument(batchSize > 0, \"batchSize must be greater than 0\");\n    if (codecName == null) {\n      codeC = null;\n      compType = CompressionType.NONE;\n    } else {\n      codeC = getCodec(codecName);\n      // TODO : set proper compression type\n      compType = CompressionType.BLOCK;\n    }\n\n    // Do not allow user to set fileType DataStream with codeC together\n    // To prevent output file with compress extension (like .snappy)\n    if (fileType.equalsIgnoreCase(HDFSWriterFactory.DataStreamType) && codecName != null) {\n      throw new IllegalArgumentException(\"fileType: \" + fileType +\n          \" which does NOT support compressed output. Please don't set codeC\" +\n          \" or change the fileType if compressed output is desired.\");\n    }\n\n    if (fileType.equalsIgnoreCase(HDFSWriterFactory.CompStreamType)) {\n      Preconditions.checkNotNull(codeC, \"It's essential to set compress codec\"\n          + \" when fileType is: \" + fileType);\n    }\n\n    // get the appropriate executor\n    this.privExecutor = FlumeAuthenticationUtil.getAuthenticator(\n            kerbConfPrincipal, kerbKeytab).proxyAs(proxyUser);\n\n    needRounding = context.getBoolean(\"hdfs.round\", false);\n\n    if (needRounding) {\n      String unit = context.getString(\"hdfs.roundUnit\", \"second\");\n      if (unit.equalsIgnoreCase(\"hour\")) {\n        this.roundUnit = Calendar.HOUR_OF_DAY;\n      } else if (unit.equalsIgnoreCase(\"minute\")) {\n        this.roundUnit = Calendar.MINUTE;\n      } else if (unit.equalsIgnoreCase(\"second\")) {\n        this.roundUnit = Calendar.SECOND;\n      } else {\n        LOG.warn(\"Rounding unit is not valid, please set one of\" +\n            \"minute, hour, or second. Rounding will be disabled\");\n        needRounding = false;\n      }\n      this.roundValue = context.getInteger(\"hdfs.roundValue\", 1);\n      if (roundUnit == Calendar.SECOND || roundUnit == Calendar.MINUTE) {\n        Preconditions.checkArgument(roundValue > 0 && roundValue <= 60,\n            \"Round value\" +\n            \"must be > 0 and <= 60\");\n      } else if (roundUnit == Calendar.HOUR_OF_DAY) {\n        Preconditions.checkArgument(roundValue > 0 && roundValue <= 24,\n            \"Round value\" +\n            \"must be > 0 and <= 24\");\n      }\n    }\n\n    this.useLocalTime = context.getBoolean(\"hdfs.useLocalTimeStamp\", false);\n    if (useLocalTime) {\n      clock = new SystemClock();\n    }\n\n    if (sinkCounter == null) {\n      sinkCounter = new SinkCounter(getName());\n    }\n  }", "signature": "void configure(Context context)", "full_signature": "@Override public void configure(Context context)", "class_method_signature": "HDFSEventSink.configure(Context context)", "testcase": false, "constructor": false, "invocations": ["checkNotNull", "getString", "getString", "getString", "getString", "getBoolean", "getString", "warn", "getName", "getString", "getString", "getTimeZone", "getLong", "getLong", "getLong", "getLong", "getInteger", "getString", "getString", "getInteger", "getLong", "getInteger", "getInteger", "getString", "getString", "getString", "getInteger", "warn", "getLong", "warn", "checkArgument", "getCodec", "equalsIgnoreCase", "equalsIgnoreCase", "checkNotNull", "proxyAs", "getAuthenticator", "getBoolean", "getString", "equalsIgnoreCase", "equalsIgnoreCase", "equalsIgnoreCase", "warn", "getInteger", "checkArgument", "checkArgument", "getBoolean", "getName"]}, "repository": {"repo_id": 2198510, "url": "https://github.com/apache/flume", "language": "Java", "is_fork": false, "fork_count": 1372, "stargazer_count": 1971, "size": 44377, "license": "licensed"}}