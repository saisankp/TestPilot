{"test_class": {"identifier": "DirectBigQueryInputFormatTest", "superclass": "", "interfaces": "", "fields": [{"original_string": "@Mock private BigQueryHelper bqHelper;", "modifier": "@Mock private", "type": "BigQueryHelper", "declarator": "bqHelper", "var_name": "bqHelper"}, {"original_string": "@Mock private BigQueryStorageClient bqClient;", "modifier": "@Mock private", "type": "BigQueryStorageClient", "declarator": "bqClient", "var_name": "bqClient"}, {"original_string": "@Mock private TaskAttemptContext taskContext;", "modifier": "@Mock private", "type": "TaskAttemptContext", "declarator": "taskContext", "var_name": "taskContext"}, {"original_string": "private JobConf config;", "modifier": "private", "type": "JobConf", "declarator": "config", "var_name": "config"}, {"original_string": "private DirectBigQueryInputFormat input;", "modifier": "private", "type": "DirectBigQueryInputFormat", "declarator": "input", "var_name": "input"}, {"original_string": "private TableReference tableRef;", "modifier": "private", "type": "TableReference", "declarator": "tableRef", "var_name": "tableRef"}, {"original_string": "private String jobProjectId = \"foo-project\";", "modifier": "private", "type": "String", "declarator": "jobProjectId = \"foo-project\"", "var_name": "jobProjectId"}, {"original_string": "private String dataProjectId = \"publicdata\";", "modifier": "private", "type": "String", "declarator": "dataProjectId = \"publicdata\"", "var_name": "dataProjectId"}, {"original_string": "private String datasetId = \"test_dataset\";", "modifier": "private", "type": "String", "declarator": "datasetId = \"test_dataset\"", "var_name": "datasetId"}, {"original_string": "private String tableId = \"test_table\";", "modifier": "private", "type": "String", "declarator": "tableId = \"test_table\"", "var_name": "tableId"}], "file": "bigquery/src/test/java/com/google/cloud/hadoop/io/bigquery/DirectBigQueryInputFormatTest.java"}, "test_case": {"identifier": "getSplits", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void getSplits() throws IOException {\n    JobContext jobContext = new JobContextImpl(config, new JobID());\n\n    CreateReadSessionRequest request =\n        CreateReadSessionRequest.newBuilder()\n            .setTableReference(\n                TableReferenceProto.TableReference.newBuilder()\n                    .setProjectId(\"publicdata\")\n                    .setDatasetId(\"test_dataset\")\n                    .setTableId(\"test_table\"))\n            .setRequestedStreams(3) // request 3, but only get 2 back\n            .setParent(\"projects/foo-project\")\n            .setReadOptions(\n                TableReadOptions.newBuilder()\n                    .addAllSelectedFields(ImmutableList.of(\"foo\", \"bar\"))\n                    .setRowRestriction(\"foo == 0\")\n                    .build())\n            .setFormat(DataFormat.AVRO)\n            .build();\n\n    ReadSession session =\n        ReadSession.newBuilder()\n            .setAvroSchema(AvroSchema.newBuilder().setSchema(\"schema\").build())\n            .addAllStreams(\n                ImmutableList.of(\n                    Stream.newBuilder().setName(\"stream1\").build(),\n                    Stream.newBuilder().setName(\"stream2\").build()))\n            .build();\n    ImmutableList<DirectBigQueryInputSplit> expected =\n        ImmutableList.of(\n            new DirectBigQueryInputSplit(\"stream1\", \"schema\", 14),\n            new DirectBigQueryInputSplit(\"stream2\", \"schema\", 14));\n\n    when(bqClient.createReadSession(any(CreateReadSessionRequest.class))).thenReturn(session);\n    try {\n      List<InputSplit> splits = input.getSplits(jobContext);\n      assertThat(splits).containsExactlyElementsIn(expected);\n    } catch (Exception e) {\n      e.printStackTrace();\n      System.exit(1);\n    }\n\n    verify(bqHelper).getTable(tableRef);\n    verify(bqClient).createReadSession(request);\n  }", "signature": "void getSplits()", "full_signature": "@Test public void getSplits()", "class_method_signature": "DirectBigQueryInputFormatTest.getSplits()", "testcase": true, "constructor": false, "invocations": ["build", "setFormat", "setReadOptions", "setParent", "setRequestedStreams", "setTableReference", "newBuilder", "setTableId", "setDatasetId", "setProjectId", "newBuilder", "build", "setRowRestriction", "addAllSelectedFields", "newBuilder", "of", "build", "addAllStreams", "setAvroSchema", "newBuilder", "build", "setSchema", "newBuilder", "of", "build", "setName", "newBuilder", "build", "setName", "newBuilder", "of", "thenReturn", "when", "createReadSession", "any", "getSplits", "containsExactlyElementsIn", "assertThat", "printStackTrace", "exit", "getTable", "verify", "createReadSession", "verify"]}, "focal_class": {"identifier": "DirectBigQueryInputFormat", "superclass": "extends InputFormat<NullWritable, GenericRecord>", "interfaces": "", "fields": [{"original_string": "private static final HadoopConfigurationProperty<Integer> DIRECT_PARALLELISM =\n      new HadoopConfigurationProperty<>(MRJobConfig.NUM_MAPS, 10);", "modifier": "private static final", "type": "HadoopConfigurationProperty<Integer>", "declarator": "DIRECT_PARALLELISM =\n      new HadoopConfigurationProperty<>(MRJobConfig.NUM_MAPS, 10)", "var_name": "DIRECT_PARALLELISM"}], "methods": [{"identifier": "getSplits", "parameters": "(JobContext context)", "modifiers": "@Override public", "return": "List<InputSplit>", "signature": "List<InputSplit> getSplits(JobContext context)", "full_signature": "@Override public List<InputSplit> getSplits(JobContext context)", "class_method_signature": "DirectBigQueryInputFormat.getSplits(JobContext context)", "testcase": false, "constructor": false}, {"identifier": "getTable", "parameters": "(Configuration configuration, BigQueryHelper bigQueryHelper)", "modifiers": "private static", "return": "Table", "signature": "Table getTable(Configuration configuration, BigQueryHelper bigQueryHelper)", "full_signature": "private static Table getTable(Configuration configuration, BigQueryHelper bigQueryHelper)", "class_method_signature": "DirectBigQueryInputFormat.getTable(Configuration configuration, BigQueryHelper bigQueryHelper)", "testcase": false, "constructor": false}, {"identifier": "startSession", "parameters": "(\n      Configuration configuration, Table table, BigQueryStorageClient client)", "modifiers": "private static", "return": "ReadSession", "signature": "ReadSession startSession(\n      Configuration configuration, Table table, BigQueryStorageClient client)", "full_signature": "private static ReadSession startSession(\n      Configuration configuration, Table table, BigQueryStorageClient client)", "class_method_signature": "DirectBigQueryInputFormat.startSession(\n      Configuration configuration, Table table, BigQueryStorageClient client)", "testcase": false, "constructor": false}, {"identifier": "createRecordReader", "parameters": "(\n      InputSplit inputSplit, TaskAttemptContext context)", "modifiers": "@Override public", "return": "RecordReader<NullWritable, GenericRecord>", "signature": "RecordReader<NullWritable, GenericRecord> createRecordReader(\n      InputSplit inputSplit, TaskAttemptContext context)", "full_signature": "@Override public RecordReader<NullWritable, GenericRecord> createRecordReader(\n      InputSplit inputSplit, TaskAttemptContext context)", "class_method_signature": "DirectBigQueryInputFormat.createRecordReader(\n      InputSplit inputSplit, TaskAttemptContext context)", "testcase": false, "constructor": false}, {"identifier": "getClient", "parameters": "(Configuration config)", "modifiers": "protected", "return": "BigQueryStorageClient", "signature": "BigQueryStorageClient getClient(Configuration config)", "full_signature": "protected BigQueryStorageClient getClient(Configuration config)", "class_method_signature": "DirectBigQueryInputFormat.getClient(Configuration config)", "testcase": false, "constructor": false}, {"identifier": "getBigQueryHelper", "parameters": "(Configuration config)", "modifiers": "protected", "return": "BigQueryHelper", "signature": "BigQueryHelper getBigQueryHelper(Configuration config)", "full_signature": "protected BigQueryHelper getBigQueryHelper(Configuration config)", "class_method_signature": "DirectBigQueryInputFormat.getBigQueryHelper(Configuration config)", "testcase": false, "constructor": false}], "file": "bigquery/src/main/java/com/google/cloud/hadoop/io/bigquery/DirectBigQueryInputFormat.java"}, "focal_method": {"identifier": "getSplits", "parameters": "(JobContext context)", "modifiers": "@Override public", "return": "List<InputSplit>", "body": "@Override\n  public List<InputSplit> getSplits(JobContext context) throws IOException {\n    final Configuration configuration = context.getConfiguration();\n    BigQueryStorageClient client = getClient(configuration);\n    BigQueryHelper bigQueryHelper;\n    try {\n      bigQueryHelper = getBigQueryHelper(configuration);\n    } catch (GeneralSecurityException gse) {\n      throw new IOException(\"Failed to create BigQuery client\", gse);\n    }\n    double skewLimit = SKEW_LIMIT.get(configuration, configuration::getDouble);\n    Preconditions.checkArgument(\n        skewLimit >= 1.0,\n        \"%s is less than 1; not all records would be read. Exiting\",\n        SKEW_LIMIT.getKey());\n    Table table = getTable(configuration, bigQueryHelper);\n    ReadSession session = startSession(configuration, table, client);\n    long numRows = table.getNumRows().longValue();\n    long limit = Math.round(skewLimit * numRows / session.getStreamsCount());\n\n    return session.getStreamsList().stream()\n        .map(\n            stream ->\n                new DirectBigQueryInputSplit(\n                    stream.getName(), session.getAvroSchema().getSchema(), limit))\n        .collect(Collectors.toList());\n  }", "signature": "List<InputSplit> getSplits(JobContext context)", "full_signature": "@Override public List<InputSplit> getSplits(JobContext context)", "class_method_signature": "DirectBigQueryInputFormat.getSplits(JobContext context)", "testcase": false, "constructor": false, "invocations": ["getConfiguration", "getClient", "getBigQueryHelper", "get", "checkArgument", "getKey", "getTable", "startSession", "longValue", "getNumRows", "round", "getStreamsCount", "collect", "map", "stream", "getStreamsList", "getName", "getSchema", "getAvroSchema", "toList"]}, "repository": {"repo_id": 19684359, "url": "https://github.com/GoogleCloudDataproc/bigdata-interop", "stars": 178, "created": "5/12/2014 3:11:55 AM +00:00", "updates": "2020-01-23T23:10:40+00:00", "fork": "False", "license": "licensed"}}