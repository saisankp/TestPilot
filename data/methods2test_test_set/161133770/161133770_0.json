{"test_class": {"identifier": "ImportJobTest", "superclass": "", "interfaces": "", "fields": [{"original_string": "private static final Logger LOGGER = LoggerFactory.getLogger(ImportJobTest.class.getName());", "modifier": "private static final", "type": "Logger", "declarator": "LOGGER = LoggerFactory.getLogger(ImportJobTest.class.getName())", "var_name": "LOGGER"}, {"original_string": "@ClassRule public static KafkaContainer kafkaContainer = new KafkaContainer();", "modifier": "@ClassRule public static", "type": "KafkaContainer", "declarator": "kafkaContainer = new KafkaContainer()", "var_name": "kafkaContainer"}, {"original_string": "@ClassRule\n  public static GenericContainer redisContainer =\n      new GenericContainer(\"redis:5.0.3-alpine\").withExposedPorts(6379);", "modifier": "@ClassRule\n  public static", "type": "GenericContainer", "declarator": "redisContainer =\n      new GenericContainer(\"redis:5.0.3-alpine\").withExposedPorts(6379)", "var_name": "redisContainer"}, {"original_string": "private static final String KAFKA_TOPIC = \"topic_1\";", "modifier": "private static final", "type": "String", "declarator": "KAFKA_TOPIC = \"topic_1\"", "var_name": "KAFKA_TOPIC"}, {"original_string": "private static final String KAFKA_SPECS_TOPIC = \"topic_specs_1\";", "modifier": "private static final", "type": "String", "declarator": "KAFKA_SPECS_TOPIC = \"topic_specs_1\"", "var_name": "KAFKA_SPECS_TOPIC"}, {"original_string": "private static final String KAFKA_SPECS_ACK_TOPIC = \"topic_specs_ack_1\";", "modifier": "private static final", "type": "String", "declarator": "KAFKA_SPECS_ACK_TOPIC = \"topic_specs_ack_1\"", "var_name": "KAFKA_SPECS_ACK_TOPIC"}, {"original_string": "private static final long KAFKA_PUBLISH_TIMEOUT_SEC = 10;", "modifier": "private static final", "type": "long", "declarator": "KAFKA_PUBLISH_TIMEOUT_SEC = 10", "var_name": "KAFKA_PUBLISH_TIMEOUT_SEC"}, {"original_string": "private static final int IMPORT_JOB_SAMPLE_FEATURE_ROW_SIZE = 128;", "modifier": "private static final", "type": "int", "declarator": "IMPORT_JOB_SAMPLE_FEATURE_ROW_SIZE = 128", "var_name": "IMPORT_JOB_SAMPLE_FEATURE_ROW_SIZE"}, {"original_string": "private static final int IMPORT_JOB_READY_DURATION_SEC = 10;", "modifier": "private static final", "type": "int", "declarator": "IMPORT_JOB_READY_DURATION_SEC = 10", "var_name": "IMPORT_JOB_READY_DURATION_SEC"}, {"original_string": "private static final int IMPORT_JOB_CHECK_INTERVAL_DURATION_SEC = 5;", "modifier": "private static final", "type": "int", "declarator": "IMPORT_JOB_CHECK_INTERVAL_DURATION_SEC = 5", "var_name": "IMPORT_JOB_CHECK_INTERVAL_DURATION_SEC"}, {"original_string": "private static final int IMPORT_JOB_MAX_RUN_DURATION_SEC = 300;", "modifier": "private static final", "type": "int", "declarator": "IMPORT_JOB_MAX_RUN_DURATION_SEC = 300", "var_name": "IMPORT_JOB_MAX_RUN_DURATION_SEC"}], "file": "ingestion/src/test/java/feast/ingestion/ImportJobTest.java"}, "test_case": {"identifier": "runPipeline_ShouldWriteToRedisCorrectlyGivenValidSpecAndFeatureRow", "parameters": "()", "modifiers": "@Test public", "return": "void", "body": "@Test\n  public void runPipeline_ShouldWriteToRedisCorrectlyGivenValidSpecAndFeatureRow()\n      throws IOException, InterruptedException {\n    Source featureSource =\n        Source.newBuilder()\n            .setType(SourceType.KAFKA)\n            .setKafkaSourceConfig(\n                KafkaSourceConfig.newBuilder()\n                    .setBootstrapServers(kafkaContainer.getBootstrapServers())\n                    .setTopic(KAFKA_TOPIC)\n                    .build())\n            .build();\n\n    IngestionJobProto.SpecsStreamingUpdateConfig specsStreamingUpdateConfig =\n        IngestionJobProto.SpecsStreamingUpdateConfig.newBuilder()\n            .setSource(\n                KafkaSourceConfig.newBuilder()\n                    .setBootstrapServers(kafkaContainer.getBootstrapServers())\n                    .setTopic(KAFKA_SPECS_TOPIC)\n                    .build())\n            .setAck(\n                KafkaSourceConfig.newBuilder()\n                    .setBootstrapServers(kafkaContainer.getBootstrapServers())\n                    .setTopic(KAFKA_SPECS_ACK_TOPIC)\n                    .build())\n            .build();\n\n    FeatureSetSpec spec =\n        FeatureSetSpec.newBuilder()\n            .setName(\"feature_set\")\n            .setProject(\"myproject\")\n            .addEntities(\n                EntitySpec.newBuilder()\n                    .setName(\"entity_id_primary\")\n                    .setValueType(Enum.INT32)\n                    .build())\n            .addEntities(\n                EntitySpec.newBuilder()\n                    .setName(\"entity_id_secondary\")\n                    .setValueType(Enum.STRING)\n                    .build())\n            .addFeatures(\n                FeatureSpec.newBuilder()\n                    .setName(\"feature_1\")\n                    .setValueType(Enum.STRING_LIST)\n                    .build())\n            .addFeatures(\n                FeatureSpec.newBuilder().setName(\"feature_2\").setValueType(Enum.STRING).build())\n            .addFeatures(\n                FeatureSpec.newBuilder().setName(\"feature_3\").setValueType(Enum.INT64).build())\n            .setSource(featureSource)\n            .build();\n\n    FeatureSet featureSet = FeatureSet.newBuilder().setSpec(spec).build();\n\n    Store redis =\n        Store.newBuilder()\n            .setName(StoreType.REDIS.toString())\n            .setType(StoreType.REDIS)\n            .setRedisConfig(\n                RedisConfig.newBuilder()\n                    .setHost(redisContainer.getHost())\n                    .setPort(redisContainer.getFirstMappedPort())\n                    .build())\n            .addSubscriptions(\n                Subscription.newBuilder()\n                    .setProject(spec.getProject())\n                    .setName(spec.getName())\n                    .build())\n            .build();\n\n    ImportOptions options = PipelineOptionsFactory.create().as(ImportOptions.class);\n\n    options.setSpecsStreamingUpdateConfigJson(\n        JsonFormat.printer().print(specsStreamingUpdateConfig));\n    options.setSourceJson(JsonFormat.printer().print(featureSource));\n    options.setStoresJson(Collections.singletonList(JsonFormat.printer().print(redis)));\n    options.setDefaultFeastProject(\"myproject\");\n    options.setProject(\"\");\n    options.setBlockOnRun(false);\n\n    List<Pair<String, FeatureRow>> input = new ArrayList<>();\n    Map<RedisKey, FeatureRow> expected = new HashMap<>();\n\n    LOGGER.info(\"Generating test data ...\");\n    IntStream.range(0, IMPORT_JOB_SAMPLE_FEATURE_ROW_SIZE)\n        .forEach(\n            i -> {\n              FeatureRow randomRow = TestUtil.createRandomFeatureRow(featureSet.getSpec());\n              RedisKey redisKey = TestUtil.createRedisKey(featureSet.getSpec(), randomRow);\n              input.add(Pair.of(\"\", randomRow));\n              List<FieldProto.Field> fields =\n                  randomRow.getFieldsList().stream()\n                      .filter(\n                          field ->\n                              spec.getFeaturesList().stream()\n                                  .map(FeatureSpec::getName)\n                                  .collect(Collectors.toList())\n                                  .contains(field.getName()))\n                      .map(\n                          field ->\n                              field.toBuilder().setName(TestUtil.hash(field.getName())).build())\n                      .collect(Collectors.toList());\n              randomRow =\n                  randomRow\n                      .toBuilder()\n                      .clearFields()\n                      .addAllFields(fields)\n                      .clearFeatureSet()\n                      .build();\n              expected.put(redisKey, randomRow);\n            });\n\n    LOGGER.info(\"Starting Import Job with the following options: {}\", options.toString());\n    PipelineResult pipelineResult = ImportJob.runPipeline(options);\n    Thread.sleep(Duration.standardSeconds(IMPORT_JOB_READY_DURATION_SEC).getMillis());\n    assertThat(pipelineResult.getState(), equalTo(State.RUNNING));\n\n    LOGGER.info(\"Publishing {} Feature Row messages to Kafka ...\", input.size());\n    TestUtil.publishToKafka(\n        kafkaContainer.getBootstrapServers(),\n        KAFKA_SPECS_TOPIC,\n        ImmutableList.of(Pair.of(getFeatureSetStringRef(spec), spec)),\n        ByteArraySerializer.class,\n        KAFKA_PUBLISH_TIMEOUT_SEC);\n    TestUtil.publishToKafka(\n        kafkaContainer.getBootstrapServers(),\n        KAFKA_TOPIC,\n        input,\n        ByteArraySerializer.class,\n        KAFKA_PUBLISH_TIMEOUT_SEC);\n    TestUtil.waitUntilAllElementsAreWrittenToStore(\n        pipelineResult,\n        Duration.standardSeconds(IMPORT_JOB_MAX_RUN_DURATION_SEC),\n        Duration.standardSeconds(IMPORT_JOB_CHECK_INTERVAL_DURATION_SEC));\n\n    LOGGER.info(\"Validating the actual values written to Redis ...\");\n    RedisClient redisClient =\n        RedisClient.create(\n            new RedisURI(\n                redisContainer.getHost(),\n                redisContainer.getFirstMappedPort(),\n                java.time.Duration.ofMillis(2000)));\n    StatefulRedisConnection<byte[], byte[]> connection = redisClient.connect(new ByteArrayCodec());\n    RedisCommands<byte[], byte[]> sync = connection.sync();\n    for (Map.Entry<RedisKey, FeatureRow> entry : expected.entrySet()) {\n      RedisKey key = entry.getKey();\n      FeatureRow expectedValue = entry.getValue();\n\n      // Ensure ingested key exists.\n      byte[] actualByteValue = sync.get(key.toByteArray());\n      assertThat(\"Key not found in Redis: \" + key, actualByteValue, notNullValue());\n\n      // Ensure value is a valid serialized FeatureRow object.\n      FeatureRow actualValue = null;\n      actualValue = FeatureRow.parseFrom(actualByteValue);\n\n      // Ensure the retrieved FeatureRow is equal to the ingested FeatureRow.\n      assertThat(actualValue, equalTo(expectedValue));\n    }\n    redisClient.shutdown();\n  }", "signature": "void runPipeline_ShouldWriteToRedisCorrectlyGivenValidSpecAndFeatureRow()", "full_signature": "@Test public void runPipeline_ShouldWriteToRedisCorrectlyGivenValidSpecAndFeatureRow()", "class_method_signature": "ImportJobTest.runPipeline_ShouldWriteToRedisCorrectlyGivenValidSpecAndFeatureRow()", "testcase": true, "constructor": false, "invocations": ["build", "setKafkaSourceConfig", "setType", "newBuilder", "build", "setTopic", "setBootstrapServers", "newBuilder", "getBootstrapServers", "build", "setAck", "setSource", "newBuilder", "build", "setTopic", "setBootstrapServers", "newBuilder", "getBootstrapServers", "build", "setTopic", "setBootstrapServers", "newBuilder", "getBootstrapServers", "build", "setSource", "addFeatures", "addFeatures", "addFeatures", "addEntities", "addEntities", "setProject", "setName", "newBuilder", "build", "setValueType", "setName", "newBuilder", "build", "setValueType", "setName", "newBuilder", "build", "setValueType", "setName", "newBuilder", "build", "setValueType", "setName", "newBuilder", "build", "setValueType", "setName", "newBuilder", "build", "setSpec", "newBuilder", "build", "addSubscriptions", "setRedisConfig", "setType", "setName", "newBuilder", "toString", "build", "setPort", "setHost", "newBuilder", "getHost", "getFirstMappedPort", "build", "setName", "setProject", "newBuilder", "getProject", "getName", "as", "create", "setSpecsStreamingUpdateConfigJson", "print", "printer", "setSourceJson", "print", "printer", "setStoresJson", "singletonList", "print", "printer", "setDefaultFeastProject", "setProject", "setBlockOnRun", "info", "forEach", "range", "createRandomFeatureRow", "getSpec", "createRedisKey", "getSpec", "add", "of", "collect", "map", "filter", "stream", "getFieldsList", "contains", "collect", "map", "stream", "getFeaturesList", "toList", "getName", "build", "setName", "toBuilder", "hash", "getName", "toList", "build", "clearFeatureSet", "addAllFields", "clearFields", "toBuilder", "put", "info", "toString", "runPipeline", "sleep", "getMillis", "standardSeconds", "assertThat", "getState", "equalTo", "info", "size", "publishToKafka", "getBootstrapServers", "of", "of", "getFeatureSetStringRef", "publishToKafka", "getBootstrapServers", "waitUntilAllElementsAreWrittenToStore", "standardSeconds", "standardSeconds", "info", "create", "getHost", "getFirstMappedPort", "ofMillis", "connect", "sync", "entrySet", "getKey", "getValue", "get", "toByteArray", "assertThat", "notNullValue", "parseFrom", "assertThat", "equalTo", "shutdown"]}, "focal_class": {"identifier": "ImportJob", "superclass": "", "interfaces": "", "fields": [{"original_string": "private static final TupleTag<FeatureRow> FEATURE_ROW_OUT = new TupleTag<FeatureRow>() {};", "modifier": "private static final", "type": "TupleTag<FeatureRow>", "declarator": "FEATURE_ROW_OUT = new TupleTag<FeatureRow>() {}", "var_name": "FEATURE_ROW_OUT"}, {"original_string": "private static final TupleTag<FailedElement> DEADLETTER_OUT = new TupleTag<FailedElement>() {};", "modifier": "private static final", "type": "TupleTag<FailedElement>", "declarator": "DEADLETTER_OUT = new TupleTag<FailedElement>() {}", "var_name": "DEADLETTER_OUT"}, {"original_string": "private static final Logger log = org.slf4j.LoggerFactory.getLogger(ImportJob.class);", "modifier": "private static final", "type": "Logger", "declarator": "log = org.slf4j.LoggerFactory.getLogger(ImportJob.class)", "var_name": "log"}], "methods": [{"identifier": "main", "parameters": "(String[] args)", "modifiers": "public static", "return": "void", "signature": "void main(String[] args)", "full_signature": "public static void main(String[] args)", "class_method_signature": "ImportJob.main(String[] args)", "testcase": false, "constructor": false}, {"identifier": "runPipeline", "parameters": "(ImportOptions options)", "modifiers": "@SuppressWarnings(\"UnusedReturnValue\") public static", "return": "PipelineResult", "signature": "PipelineResult runPipeline(ImportOptions options)", "full_signature": "@SuppressWarnings(\"UnusedReturnValue\") public static PipelineResult runPipeline(ImportOptions options)", "class_method_signature": "ImportJob.runPipeline(ImportOptions options)", "testcase": false, "constructor": false}], "file": "ingestion/src/main/java/feast/ingestion/ImportJob.java"}, "focal_method": {"identifier": "runPipeline", "parameters": "(ImportOptions options)", "modifiers": "@SuppressWarnings(\"UnusedReturnValue\") public static", "return": "PipelineResult", "body": "@SuppressWarnings(\"UnusedReturnValue\")\n  public static PipelineResult runPipeline(ImportOptions options) throws IOException {\n    /*\n     * Steps:\n     * 1. Read FeatureSetSpec messages from kafka\n     * 2. Read messages from Feast Source as FeatureRow\n     * 3. Validate the feature rows to ensure the schema matches what is registered to the system\n     * 4. Distribute rows across stores by subscription\n     * 5. Write FeatureRow to the corresponding Store\n     * 6. Write elements that failed to be processed to a dead letter queue.\n     * 7. Write metrics to a metrics sink\n     * 8. Send ack on receiving FeatureSetSpec\n     */\n\n    PipelineOptionsValidator.validate(ImportOptions.class, options);\n    Pipeline pipeline = Pipeline.create(options);\n\n    log.info(\"Starting import job with settings: \\n{}\", options.toString());\n\n    List<Store> stores = SpecUtil.parseStoreJsonList(options.getStoresJson());\n    Source source = SpecUtil.parseSourceJson(options.getSourceJson());\n    SpecsStreamingUpdateConfig specsStreamingUpdateConfig =\n        SpecUtil.parseSpecsStreamingUpdateConfig(options.getSpecsStreamingUpdateConfigJson());\n\n    // Step 1. Read FeatureSetSpecs from Spec source\n    PCollection<KV<FeatureSetReference, FeatureSetSpec>> featureSetSpecs =\n        pipeline.apply(\n            \"ReadFeatureSetSpecs\",\n            ReadFeatureSetSpecs.newBuilder()\n                .setSource(source)\n                .setStores(stores)\n                .setSpecsStreamingUpdateConfig(specsStreamingUpdateConfig)\n                .build());\n\n    PCollectionView<Map<String, Iterable<FeatureSetSpec>>> globalSpecView =\n        featureSetSpecs\n            .apply(MapElements.via(new ReferenceToString()))\n            .apply(\"GlobalSpecView\", View.asMultimap());\n\n    // Step 2. Read messages from Feast Source as FeatureRow.\n    PCollectionTuple convertedFeatureRows =\n        pipeline.apply(\n            \"ReadFeatureRowFromSource\",\n            ReadFromSource.newBuilder()\n                .setSource(source)\n                .setSuccessTag(FEATURE_ROW_OUT)\n                .setFailureTag(DEADLETTER_OUT)\n                .setKafkaConsumerProperties(\n                    options.getKafkaConsumerProperties() == null\n                        ? new HashMap<>()\n                        : options.getKafkaConsumerProperties())\n                .build());\n\n    // Step 3. Process and validate incoming FeatureRows\n    PCollectionTuple validatedRows =\n        convertedFeatureRows\n            .get(FEATURE_ROW_OUT)\n            .apply(\n                ProcessAndValidateFeatureRows.newBuilder()\n                    .setDefaultProject(options.getDefaultFeastProject())\n                    .setFeatureSetSpecs(globalSpecView)\n                    .setSuccessTag(FEATURE_ROW_OUT)\n                    .setFailureTag(DEADLETTER_OUT)\n                    .build());\n\n    Map<Store, TupleTag<FeatureRow>> storeTags =\n        stores.stream()\n            .map(s -> Pair.of(s, new TupleTag<FeatureRow>()))\n            .collect(Collectors.toMap(Pair::getLeft, Pair::getRight));\n\n    // Step 4. Allocate validated rows to stores by store subscription\n    PCollectionTuple storeAllocatedRows =\n        validatedRows\n            .get(FEATURE_ROW_OUT)\n            .apply(\n                FeatureRowToStoreAllocator.newBuilder()\n                    .setStores(stores)\n                    .setStoreTags(storeTags)\n                    .build());\n\n    PCollectionList<FeatureSetReference> sinkReadiness = PCollectionList.empty(pipeline);\n\n    for (Store store : stores) {\n      FeatureSink featureSink = getFeatureSink(store);\n\n      sinkReadiness = sinkReadiness.and(featureSink.prepareWrite(featureSetSpecs));\n      PCollection<FeatureRow> rowsForStore =\n          storeAllocatedRows.get(storeTags.get(store)).setCoder(ProtoCoder.of(FeatureRow.class));\n\n      // Step 5. Write metrics of successfully validated rows\n      rowsForStore.apply(\n          \"WriteInflightMetrics\", WriteInflightMetricsTransform.create(store.getName()));\n\n      // Step 6. Write FeatureRow to the corresponding Store.\n      WriteResult writeFeatureRows =\n          rowsForStore.apply(\"WriteFeatureRowToStore\", featureSink.writer());\n\n      // Step 7. Write FailedElements to a dead letter table in BigQuery.\n      if (options.getDeadLetterTableSpec() != null) {\n        // TODO: make deadletter destination type configurable\n        DeadletterSink deadletterSink =\n            new BigQueryDeadletterSink(options.getDeadLetterTableSpec());\n\n        writeFeatureRows\n            .getFailedInserts()\n            .apply(\"WriteFailedElements_WriteFeatureRowToStore\", deadletterSink.write());\n      }\n\n      // Step 8. Write metrics to a metrics sink.\n      writeFeatureRows\n          .getSuccessfulInserts()\n          .apply(\"WriteSuccessMetrics\", WriteSuccessMetricsTransform.create(store.getName()));\n\n      writeFeatureRows\n          .getFailedInserts()\n          .apply(\"WriteFailureMetrics\", WriteFailureMetricsTransform.create(store.getName()));\n    }\n\n    if (options.getDeadLetterTableSpec() != null) {\n      DeadletterSink deadletterSink = new BigQueryDeadletterSink(options.getDeadLetterTableSpec());\n\n      convertedFeatureRows\n          .get(DEADLETTER_OUT)\n          .apply(\"WriteFailedElements_ReadFromSource\", deadletterSink.write());\n\n      validatedRows\n          .get(DEADLETTER_OUT)\n          .apply(\"WriteFailedElements_ValidateRows\", deadletterSink.write());\n    }\n\n    sinkReadiness\n        .apply(Flatten.pCollections())\n        .apply(\n            \"WriteAck\",\n            WriteFeatureSetSpecAck.newBuilder()\n                .setSinksCount(stores.size())\n                .setSpecsStreamingUpdateConfig(specsStreamingUpdateConfig)\n                .build());\n\n    return pipeline.run();\n  }", "signature": "PipelineResult runPipeline(ImportOptions options)", "full_signature": "@SuppressWarnings(\"UnusedReturnValue\") public static PipelineResult runPipeline(ImportOptions options)", "class_method_signature": "ImportJob.runPipeline(ImportOptions options)", "testcase": false, "constructor": false, "invocations": ["validate", "create", "info", "toString", "parseStoreJsonList", "getStoresJson", "parseSourceJson", "getSourceJson", "parseSpecsStreamingUpdateConfig", "getSpecsStreamingUpdateConfigJson", "apply", "build", "setSpecsStreamingUpdateConfig", "setStores", "setSource", "newBuilder", "apply", "apply", "via", "asMultimap", "apply", "build", "setKafkaConsumerProperties", "setFailureTag", "setSuccessTag", "setSource", "newBuilder", "getKafkaConsumerProperties", "getKafkaConsumerProperties", "apply", "get", "build", "setFailureTag", "setSuccessTag", "setFeatureSetSpecs", "setDefaultProject", "newBuilder", "getDefaultFeastProject", "collect", "map", "stream", "of", "toMap", "apply", "get", "build", "setStoreTags", "setStores", "newBuilder", "empty", "getFeatureSink", "and", "prepareWrite", "setCoder", "get", "get", "of", "apply", "create", "getName", "apply", "writer", "getDeadLetterTableSpec", "getDeadLetterTableSpec", "apply", "getFailedInserts", "write", "apply", "getSuccessfulInserts", "create", "getName", "apply", "getFailedInserts", "create", "getName", "getDeadLetterTableSpec", "getDeadLetterTableSpec", "apply", "get", "write", "apply", "get", "write", "apply", "apply", "pCollections", "build", "setSpecsStreamingUpdateConfig", "setSinksCount", "newBuilder", "size", "run"]}, "repository": {"repo_id": 161133770, "url": "https://github.com/gojek/feast", "stars": 633, "created": "12/10/2018 7:20:15 AM +00:00", "updates": "2020-01-27T12:39:33+00:00", "fork": "False", "license": "licensed"}}